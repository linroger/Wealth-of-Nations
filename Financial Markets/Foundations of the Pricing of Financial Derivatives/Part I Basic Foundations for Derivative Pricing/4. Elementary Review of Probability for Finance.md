# Elementary Review of Probability for Finance  

probability and statistics. Corporate cash flows, exchange rates, commodity prices, stock prices, and interest rates are all random variables that can be usefully modeled by using probability and statistics. Unfortunately, you never have the luxury of observing the actual, objective probability distribution in the social science field of finance. The closest alternative is your perceived, subjective probability distribution. Historical frequencies of an event are never an exact representation of future outcome. Thus, we must be cognizant of the well-known disclaimer, “Please remember that past performance may not be indicative of future results.” Market participants learn from the past and make behavioral adjustments that influence future observations.  

This chapter provides a brief introductory review of the key foundational concepts in probability that have proven useful in financial analysis. More depth is found in almost any good book on probability.1  

# 4.1 MARGINAL, CONDITIONAL, AND JOINT PROBABILITIES  

Probability is a measure, meaning a numerical description or quantification, of the relative frequency of an event. The probability of observing heads in one toss of a fair coin is obviously 1/2. There are two possibilities and one fits the definition of the desired event. Technically this concept is called the marginal probability, which distinguishes it from the conditional probability, which is the probability that something will happen, given that something else has already happened. Finally, we have the concept of joint probability, which is the probability that two or more events will happen.  

To illustrate these concepts, consider the following information. A sample of 100 people, 55 female and 45 male, is collected and examined for the frequency of brown hair. Of the 55 females, 31 have brown hair and 24 have some other color. Of the 45 males, 28 have brown hair and 17 have some other color. If this is a reliable sample, what is the probability that a person selected at random will have brown hair? This is the unconditional probability, sometimes called the marginal probability, and it is 0.59, given that 59 out of 100 have brown hair. Note that we did not condition on whether the person is male or female.  

Now suppose you know that you selected a male. Then what would be the probability that the subject has brown hair? This concept is the conditional probability, specifically the probability that the subject would have brown hair, given that it is a male. Then the answer would be $28/45=0.62$ because $62\%$ of the males have brown hair.  

Now suppose you selected someone at random and you wanted to know the probability that it would be a female with a hair color other than brown. Out of 100 subjects, 24 are females with a hair color other than brown. So, on that basis alone, we know that the joint probability would be 0.24. The joint probability is equal to the conditional probability times the marginal probability of the condition. In our example this means that we want the probability that the subject will be a female with a hair color other than brown and this will be the probability that the subject will be female, given that you know the subject does not have brown hair, times the probability that the subject does not have brown hair. The probability that the subject will be female, given that we know that the subject does not have brown hair is 24/41 because there are 41 people with a hair color other than brown and 24 are female. This is the conditional probability. The probability that the subject does not have brown hair is 41/100 because you know that out of 100 people, 41 do not have brown hair. This is the marginal probability. Multiplying the conditional probability times the marginal probability gives us $(24/41)(41/100)=24/100$ .  

Alternatively, we could have found the conditional probability that the subject will not have brown hair given that we know it is a female, which will be 24/55, times the probability that the subject is female, 55/100, giving us the correct answer of 24/100.  

Now, let us consider another simple event, such as the probability of a head on one toss of a coin being 1/2. Now suppose we toss two coins. What is the probability of a head? Now we have to define more precisely what we mean. Do we mean one head, two heads, or at least one head? We also have to be assured that the coins are independent, for if not, it does affect our answer. Let us focus on the probability of exactly one head. Assuming the two coins are independent, consider the probability of one head as restricted to the probability that we toss a head on coin one and a tail on coin two, which is (1/2) $(1/2)=1/4$ . This is what we mean by the joint probability with independent events. There are two ways to toss one head, however: a head on the first and a tail on the second or a tail on the first and a head on the second. So, the probability of exactly one head is really $1/4+1/4=1/2$ . If we want the probability of two heads, then we must toss a head on both coins. Because the events are independent, the joint probability is the product of the marginal probabilities. Thus, the probability of two heads is $(1/2)(1/2)=1/4\/$ . If we want the probability of at least one head, we require the probability of exactly one head plus the probability of two heads, which is $1/2+1/4=3/4$ . Interestingly, there is another way of solving the “at least one” case. Its probability is one minus the probability of no heads. The probability of no heads is $(1/2)(1/2)=1/4\AA$ . So, the probability of at least one head is $1-1/4=3/4$ .  

We can easily verify these results by summarizing all possible outcomes:  

■ Head on first coin, head on second coin ■ Head on first coin, tail on second coin Tail on first coin, head on second coin ■ Tail on first coin, tail on second coin  

These are equally likely events so each has a probability of $1/4$ . The outcome of one head is represented by the second and third events so the probability is $1/4+1/4=1/2.$ . For two heads, there is just one way to do it, the first event. So that probability is 1/4. If we want the probability of at least one head, we include the first three events, so that probability is $1/4+1/4+1/4=3/4$ . Alternatively, the probability of at least one head can be restated as 1 minus the probability of no heads. There is only one event here with no heads, the fourth event, so the answer is $1-1/4=3/4$ or alternatively, 1 – the probability of a tail on both tosses, which is $1-1/4=3/4$ .  

The probability of two events occurring, as noted, is called the joint probability. Defining the events as $A$ and $B$ and denoting probability as $\mathrm{Pr},$ we state the joint probability for independent events in the following manner:  

$\operatorname*{Pr}(A{\mathrm{~and~}}B)=\operatorname*{Pr}(A)\operatorname*{Pr}(B).$ , given independence of $A$ and $B$ .  

For events that are dependent, we must make some modifications. First, let us ask what we mean by independence and dependence. The coin tosses are clearly independent. The outcome of one coin is unrelated to the outcome of the other. We can, however, map the coin tosses into values that are not independent. Consider the following experiment. Toss a coin one time. If heads occur, record the value $+1$ ; if tails occurs, record the value $^{-1}$ . Then toss the coin again. Once again record $+1$ or $^{-1}$ and add it to the amount recorded after the first coin toss. We are interested in the probabilities of the numerical outcomes. Here are the equally likely possibilities:  

■ Head on first coin, head on second coin: $\mathrm{sum}=+2$ ■ Head on first coin, tail on second coin: $\mathbf{sum}=0$ ■ Tail on first coin, head on second coin: $\mathsf{s u m}=0$ ■ Tail on first coin, tail on second coin: sum $=-2$  

Thus, the probability distribution is as follows:  

<html><body><table><tr><td>Outcome</td><td>Probability</td></tr><tr><td>+2</td><td>1/4</td></tr><tr><td>0</td><td>1/2</td></tr><tr><td></td><td></td></tr><tr><td>-2</td><td>1/4</td></tr></table></body></html>  

These are the marginal probabilities.2 We might also want to know the conditional probabilities. For example, we might wish to know the probabilities of $+2,0$ , and $^{-2}$ given that a head occurred on the first toss. These probabilities are as follows:  

<html><body><table><tr><td>Outcome</td><td>Probability3</td></tr><tr><td>+2</td><td>1/2</td></tr><tr><td>0</td><td>1/2</td></tr><tr><td></td><td></td></tr><tr><td>-2</td><td>0</td></tr></table></body></html>  

If a tail occurs on the first toss, the probabilities are as follows:4  

<html><body><table><tr><td>Outcome</td><td>Probability</td></tr><tr><td></td><td>0</td></tr><tr><td>+2</td><td>1/2</td></tr><tr><td>0</td><td></td></tr><tr><td>-2</td><td>1/2</td></tr></table></body></html>  

The probabilities of $+2$ and $^{-2}$ outcomes are quite different if a head or tail has occurred on the first toss. This is what we mean by the conditional probability. The events, defined specifically as the sum of the $+1$ or $^{-1}$ assigned to the head or tail tossed, are not independent. The occurrence of a head or a tail, however, is independent from one toss to the other. It is interesting to note how the same sample experiment can give rise to a quite different set of probabilities for different events.  

In general, we have the following result, where $\dot{\mathcal{O}}$ denotes and:  

$$
\operatorname*{Pr}(A\&B)=\operatorname*{Pr}(B\mid A)\operatorname*{Pr}(A)=\operatorname*{Pr}(A\mid B)\operatorname*{Pr}(B).
$$  

Note that $\mathrm{Pr}(.)$ stands for the probability of the event(s) in the parentheses. When a vertical line appears in parentheses it means conditional on the event following the vertical line. To cast our event in this context, think of the two tosses as two coins, coin1 and coin2. The outcome of a $+2$ can occur only in the joint condition that both coins come up heads. The probability of this occurring is obtained from the conditional probabilities as follows: the probability of a head on the second coin conditional on a head on the first coin $(16)$ times the probability of a head on the first coin $(16)$ . Alternatively, it is the probability of a head on the first coin conditional on a head on the second coin $(1/\l_{2})$ times the probability of a head on the second coin $(16)$ .5  

A probability distribution is a mathematical specification of the probabilities associated with events. For the examples shown, we easily laid out the probability distribution. Many types of random outcomes can be characterized with an exact mathematical formulation that gives either the probability of an event occurring or the probability of a range of events occurring. Coin tosses are outcomes from what is called a binomial probability distribution. This distribution is one of a family of distributions that are called discrete, which means that only a finite number of outcomes can occur. Another family of distributions is referred to as continuous, meaning that the number of possible outcomes is infinite. Anything that can be measured with fractional precision is continuous. For example, the return on an asset is continuous. An asset bought at 99.75 and sold at 102.5 has a return of $2.757\dots\%$ . There are an infinite number of possible returns, provided the return is measured with decimal precision and not rounded off to a certain number of decimal places. One major example of a continuous distribution is the familiar normal or bell-shaped distribution.  

Consider a random variable, $x.$ . An expression such as $\operatorname*{Pr}(x=a)$ , representing the probability that an outcome of $x$ is a value of $a$ , is one example of information revealed by a discrete probability distribution. In some cases, we wish to know the cumulative probability, $\Pr(x\leq a)$ . For example, consider the previous problem in which we wish to know the probability of achieving a specific total after two coin tosses where a head counts as $+1$ and a tail counts as $^{-1}$ . Suppose we wish to know the probability that the total is nonnegative. Then we have $\operatorname*{Pr}(x\geq0)$ , which equals $\operatorname*{Pr}(x=0)+\operatorname*{Pr}(x=2)=1/2+1/4=3/4$ . Alternatively, we could calculate using the complement, $\operatorname*{Pr}(x\ge0)=1-\operatorname*{Pr}(x<0)=1-$ $\operatorname*{Pr}(x=-2)=1-1/4=3/4$ . Note that it is not possible to obtain a total of $+1$ or $^{-1}$ . The outcomes are either $+2$ , 0, or $^{-2}$ .  

If a random variable is continuous, we cannot specify a probability in terms of a specific value. For example, in the standard normal distribution we cannot specify $\operatorname*{Pr}(x=0)$ .6 Because there are an infinite number of outcomes, the probability of any one outcome occurring is zero. We can, however, specify the probability of a range of outcomes occurring. Statements like $\operatorname*{Pr}(x>0)$ are quite acceptable.7 The answer is 0.5, owing to the symmetry of the distribution and the fact that its expected value is zero. Likewise, we can use statements like $\operatorname*{Pr}(b<x<a)$ , which is easily found as $\operatorname*{Pr}(x<a)-\operatorname*{Pr}(x<b)$ .  

Because the probability of a specific value of the random variable occurring is zero in the continuous case, there is no mathematical specification that gives such results as $\operatorname*{Pr}(x=a)$ . There is, however, a mathematical specification that can lead to such statements as $\operatorname*{Pr}(x<a)$ . We start with a mathematical function referred to as the probability density function. If we plot such a function, specified as $f(x)$ , we observe a graph of values of a variable $f(x)$ in terms of the random variable $X$ . The specific value of $f(x)$ has no particular interpretation in a continuous distribution, but the area under the curve generated by $f(x)$ is the probability we seek. Thus, the area under the curve and to the left of a particular value $f(a)$ is $\operatorname*{Pr}(x<a)$ . Integrating the function, meaning to accumulate its values over a range of values of $x$ , gives us the desired probability. These concepts are discussed in more detail later in this chapter. Such a function is called the probability distribution function, or just the distribution function, and sometimes the cumulative density function.8  

Probability distributions are often characterized not only with a density or distribution function but also with a moment-generating function. This is a mathematical specification that incorporates the density or distribution function and yields what are called the moments of the distribution. The $k^{t h}$ raw moment of a distribution is defined as $E{\big(}x^{k}{\big)}$ . As we shall see in the next section, the first moment $\begin{array}{r}{\left(k=1\right)}\end{array}$ ) is the expected value and the second moment $(k=2)$ is closely related to the variance. The third moment, $E\big(x^{3}\big)$ , is closely related to the concept of skewness, which measures the symmetry of a probability distribution. The fourth moment, $E\left(x^{4}\right)$ , is closely related to the concept of kurtosis, which measures the extent to which a distribution is peaked or flat.  

Not all probability distributions have a moment-generating function, but all have a characteristic function, which, although requiring use of complex numbers, can be used to yield many useful results. Characteristic functions and to some extent moment-generating functions are occasionally used in finance.  

These paragraphs provide only a brief treatment but should be sufficient to refresh our memory of previous encounters with this material. In some cases, certain concepts are being encountered for the first time. The reader will in all likelihood be required to refer to more specific material to fill in gaps and extend knowledge. We now turn to the primary operations used with random variables in finance concepts, which are the determination of expectations, variances, and covariances.  

# 4.2 EXPECTATIONS, VARIANCES, AND COVARIANCES OF DISCRETE RANDOM VARIABLES  

A random variable is a variable that can take on many possible values representing uncertain outcomes whose frequencies of occurrence are governed by a probability distribution. A discrete random variable can take on only a finite number of values. For example, the number of people who respond yes to a survey or the number of laboratory mice who died following an experiment are examples of a discrete random variable. By contrast, a continuous random variable can take on an infinite number of values. For example, the height of a person selected randomly or the amount of time following an event can always be expressed with decimal places. For however many decimal places chosen, one can always add one more. Let us look at the expectations, variances, and covariances of both of these types of variables. We address discrete random variables first.  

For a discrete random variable there are a finite number of outcomes that we often call states or states of the world. For example, if the event is the selection of a person and we are interested in whether that person is male or female, we would have two states. Let $x=x_{m}$ if that person is male and $x=x_{f}$ if that person is female. In general, we specify $n$ states and $n$ possible values of $x:x_{1},x_{2},\ldots,x_{n}$ . We often need to characterize the properties of this random variable $x$ . We let $p_{i}$ be the probability that state or outcome $i$ occurs. Note that by definition $\sum_{i=1}^{n}{p_{i}=1}$ . It may well be the case that ${{p}_{i}}$ is given by some mathematical function that might more appropriately be written as $f(x)$ , but here we shall just leave the probability specification in the form $p_{i}$ .  

# 4.2.1 The Expectation of a Discrete Random Variable  

The expected value, sometimes called the mean, is the probability-weighted average value of $x$ .9 The expected value for a discrete distribution is the following specification:  

$$
E(x)=\sum_{i=1}^{n}x_{i}p_{i}.
$$  

Occasionally we shall have to work with a constant such as $a$ in the following operations:  

$$
E(a)=\sum_{i=1}^{n}a p_{i}=a\sum_{i=1}^{n}{p_{i}}=a.
$$  

The expected value of a constant is simply the constant. Next, we multiply the constant times the random variable and are able to pull the constant out of the expectation sign,  

$$
E(a x)=a\sum_{i=1}^{n}x_{i}\slash p_{i}=a E(x).
$$  

The Greek letter $\mu$ is often used for the expected value, but other Greek letters and many other symbols are also sometimes used as the expected value.  

For example, Table 4.1 provides an analyst’s forecast of potential rates of return from holding a particular tech company stock over the next year.  

TABLE 4.1 Potential Rates of Return   


<html><body><table><tr><td>Probability (%)</td><td>Rate of Return (%)</td></tr><tr><td>10</td><td>-25</td></tr><tr><td>20</td><td>-5</td></tr><tr><td>40</td><td>10</td></tr><tr><td>20</td><td>20</td></tr><tr><td>10</td><td>55</td></tr></table></body></html>  

Based on Equation (4.1), we have  

$$
E(R)=\sum_{i=1}^{n}R_{i}{p}_{i}=-25(0.1)-5(0.2)+10(0.4)+20(0.2)+55(0.1)=10,
$$  

or an expected rate of return of $10\%$ .  

# 4.2.2 The Variance of a Discrete Random Variable  

The variance is a measure of the dispersion of the distribution. It is defined as the probability-weighted squared deviation of each possible value from the expected value. Using that construction, we see that the variance can be converted to another useful specification:  

$$
\begin{array}{l}{{\displaystyle\operatorname{var}(x)=\sum_{i=1}^{n}\left[x_{i}-E(x)\right]^{2}\dot{p_{i}}=\sum_{i=1}^{n}\left[x_{i}-E(x)\right]\left[x_{i}-E(x)\right]\dot{p_{i}}}}\\ {~=\displaystyle\sum_{i=1}^{n}\left\{x_{i}^{2}-2E(x)x_{i}+[E(x)]^{2}\right\}\dot{p_{i}}=\sum_{i=1}^{n}x_{i}^{2}\dot{p_{i}}-2E(x)\sum_{i=1}^{n}x_{i}\dot{p_{i}}+[E(x)]^{2}\sum_{i=1}^{n}p_{i}}\\ {~=E\big(x^{2}\big)-2[E(x)]^{2}+[E(x)]^{2}=E\big(x^{2}\big)-[E(x)]^{2}.}\end{array}
$$  

In other words, the variance is also the expected value of the squared value of $x$ minus the square of the expected value of $x$ .  

When we work with constants, the variance is affected in the following manner:  

$$
\begin{array}{l}{\displaystyle\mathrm{var}(a x)=\sum_{i=1}^{n}\big[a x_{i}-E(a x)\big]^{2}p_{i}=E\left[(a x)^{2}\right]-[E(a x)]^{2}=E\left(a^{2}x^{2}\right)-a^{2}[E(x)]^{2}}\\ {\displaystyle=a^{2}E\big(x^{2}\big)-a^{2}[E(x)]^{2}=a^{2}\left\{E\big(x^{2}\big)-[E(x)]^{2}\right\}=a^{2}V a r(x).}\end{array}
$$  

In other words, the variance of a constant times a random variable is the constant squared times the variance of the random variable. The variance of a constant is zero as shown by the following:  

$$
\operatorname{var}(a)=\sum_{i=1}^{n}\left[a-E(a)\right]^{2}{p_{i}}=\sum_{i=1}^{n}(a-a)^{2}{p_{i}}=0.
$$  

Finally, we also show that the variance of a constant plus a random variable is the variance of the random variable. In other words, adding a constant does nothing to change the variance of the random variable.  

$$
\begin{array}{l}{\displaystyle\operatorname{var}(a+x)=\sum_{i=1}^{n}\left[a+x_{i}-E(a+x)\right]^{2}\boldsymbol{\gamma}_{i}}\\ {\displaystyle=\sum_{i=1}^{n}\left[a-E(a)+x_{i}-E(x)\right]^{2}\boldsymbol{\gamma}_{i}=\sum_{i=1}^{n}\left[a-a+x_{i}-E(x)\right]^{2}\boldsymbol{\gamma}_{i}}\\ {\displaystyle=\sum_{i=1}^{n}\left[x_{i}-E(x)\right]^{2}\boldsymbol{\gamma}_{i}}\\ {\displaystyle=V a r(x).}\end{array}
$$  

The square root of the variance is the standard deviation. Using $\sigma$ as its symbol, we have $\sigma(a x)=a(\sigma(x))$ .  

Based on the same data in the last example, Table 4.2 provides supplemental information to assist in the calculation of the standard deviation. Based on Equation (4.4), we have  

$$
\operatorname{var}\left(R\right)=E\left(R^{2}\right)-\left[E(R)\right]^{2}=490-10^{2}=390,
$$  

or a standard deviation of $19.748\%\left(={\sqrt{390}}\right)$  

# 4.2.3 The Covariance of Discrete Random Variables  

An important concept somewhat similar to the variance is the covariance. It measures the extent to which two random variables move together. Now we need another random variable, which we shall refer to as $y$ . The covariance between $x$ and $y$ is given as  

$$
{\begin{array}{l}{{\displaystyle\operatorname{cov}(x,y)=\sum_{i=1}^{n}\left[x_{i}-E(x)\right]\left[y_{i}-E(y)\right]{\dot{p}}_{i}=\sum_{i=1}^{n}\left[x_{i}y_{i}-E(x)y_{i}-x_{i}E(y)+E(x)E(y)\right]p_{i}}}\\ {\displaystyle\qquad=\sum_{i=1}^{n}x_{i}y_{i}{\dot{p}}_{i}-E(x)\sum_{i=1}^{n}y_{i}{\dot{p}}_{i}-E(y)\sum_{i=1}^{n}x_{i}{\dot{p}}_{i}+E(x)E(y)\sum_{i=1}^{n}p_{i}}\\ {\displaystyle\qquad=E(x y)-E(x)E(y)-E(y)E(x)+E(x)E(y)}\\ {\displaystyle\qquad=E(x y)-E(x)E(y).}\end{array}}
$$  

TABLE 4.2 Illustration of Supplemental Information to Compute Standard Deviation   


<html><body><table><tr><td>Probability (%)</td><td>Rate of Return (%)</td><td>Rate of Return2</td></tr><tr><td>10</td><td>-25</td><td>625</td></tr><tr><td>20</td><td>-5</td><td>25</td></tr><tr><td>40</td><td>10</td><td>100</td></tr><tr><td>20</td><td>20</td><td>400</td></tr><tr><td>10</td><td>55</td><td>3,025</td></tr><tr><td>Sum = 100%</td><td>E(R)= 10%</td><td>E(R²)= 490</td></tr></table></body></html>  

The numerical value of the covariance by itself is difficult to interpret. For example, we do not know what constitutes a large covariance, and what is a small covariance. We do know that a positive (negative) covariance means that the two variables tend to move together (opposite) in a linear fashion. A zero covariance implies that there is no linear relationship between the two variables, but it does not rule out a nonlinear relationship.10 A more useful measure of association is the correlation, defined as  

$$
\rho(x,y)=\frac{\mathrm{cov}(x,y)}{\sigma(x)\sigma(y)},
$$  

where $\sigma(x)$ and $\sigma(y)$ are the standard deviations of $x$ and $y$ , respectively. The correlation ranges between $^{-1}$ and $+1$ . With bounds at $^{-1}$ and $+1$ , it is a good bit easier to interpret the magnitude of a correlation.  

Based on the data in the last example along with a second instrument, Table 4.3 provides supplemental information to assist in the calculation of covariance. Based on Equation (4.8), we have the covariance as  

$$
\operatorname{cov}(R_{1},R_{2})=E(R_{1}R_{2})-E(R_{1})E(R_{2})=535-10(10)=435.
$$  

Note that interpreting 435 is difficult. Is it large or small? Also, note,  

$$
\begin{array}{l}{\operatorname{var}(R_{1})=490-10^{2}=390~\mathrm{and}}\\ {\qquad}\\ {\operatorname{var}(R_{2})=1{,}090-10^{2}=990.}\end{array}
$$  

The correlation coefficient, however, is more intuitive as  

$$
\rho(R_{1},R_{2})=\frac{\mathrm{cov}(R_{1},R_{2})}{\sigma(R_{1})\sigma(R_{2})}=\frac{435}{\sqrt{390}\sqrt{990}}=\frac{435}{19.748(31.464)}=0.7.
$$  

A correlation of 0.7 is a strong positive correlation.  

An important result that is sometimes seen in finance is that the covariance of a variable with itself is the variance. We can easily see this by letting $y$ also be $x$ and thereby obtaining the covariance of $x$ with itself:  

$$
\operatorname{cov}(x,y|y=x)=\operatorname{cov}(x,x)=\sum_{i=1}^{n}{\big[}x_{i}-E(x){\big]}{\big[}x_{i}-E(x){\big]}\ p_{i}=V a r(x).
$$  

TABLE 4.3 Illustration of Supplemental Information to Compute Covariance   


<html><body><table><tr><td>Probability (%)</td><td>R(1) (%)</td><td>R(1)2</td><td>R(2) (%)</td><td>R(2)²</td><td>R(1)R(2)</td></tr><tr><td>10%</td><td>-25</td><td>625</td><td>20</td><td>400</td><td>-500</td></tr><tr><td>20%</td><td>-5</td><td>25</td><td>-15</td><td>225</td><td>75</td></tr><tr><td>40%</td><td>10</td><td>100</td><td>0</td><td>0</td><td>0</td></tr><tr><td>20%</td><td>20</td><td>400</td><td>5</td><td>25</td><td>100</td></tr><tr><td>10%</td><td>55</td><td>3,025</td><td>100</td><td>10,000</td><td>5,500</td></tr><tr><td>Sum = 100%</td><td>E(R)= 10%</td><td>E(R²)=490</td><td>E(R)= 10%</td><td>E(R²)= 1,090</td><td>E[R(1)R(2)]= 535</td></tr></table></body></html>  

We also need to know that the covariance of a random variable and a constant is zero:  

$$
\operatorname{cov}(x,a)=\sum_{i=1}^{n}{\big[}x_{i}-E(x){\big]}[a-E(a)]\ p_{i}=\sum_{i=1}^{n}{\big[}x_{i}-E(x){\big]}(a-a)p_{i}=0.
$$  

The covariance concept facilitates the understanding of the variance of a combination of more than one random variable. Consider a weighted sum of variables $x$ and $y_{;}$ , obtained by multiplying $x$ by $a$ and $\mathrm{\Deltay}$ by $b$ . Suppose we wish to find the variance of $a x+b y$ :  

$$
\begin{array}{r l}{\langle\cos-i p\rangle-\displaystyle\sum_{j=1}^{N}\Big(i\alpha_{j}+i\alpha_{j}\Big)^{2}\Big(i\alpha_{j}-i\alpha_{j}\Big)^{2}\Big(i\alpha_{j}-i\alpha_{j}+i\alpha_{j}\Big)^{2}\Bigg)^{\theta}}&{=\Big[\displaystyle\sum_{j=1}^{N}\alpha_{j}+i\alpha_{j}+i\alpha_{j}\Big]^{2}}\\ &{\quad-\displaystyle\sum_{j=1}^{N}\Big(i\alpha_{j}+i\alpha_{j}\Big)[2i\alpha_{j}-i\alpha_{j}\Big)^{2}\Big(i\alpha_{j}+i\alpha_{j}-i\alpha_{j}\Big)^{2}\Big(i\alpha_{j}+i\alpha_{j}\Big)^{2}}\\ &{=\displaystyle\sum_{j=1}^{N}i\alpha_{j}+i\alpha_{j}\Big(i\alpha_{j}+i\alpha_{j}\Big)^{2}\Big(i\alpha_{j}-i\alpha_{j}+i\alpha_{j}\Big)^{2}}\\ &{=\displaystyle-\sum_{j=1}^{N}i\alpha_{j}+i\alpha_{j}\Big(i\alpha_{j}+i\alpha_{j}\Big)^{2}\Big(i\alpha_{j}+i\alpha_{j}-i\alpha_{j}\Big)^{2}}\\ &{=\displaystyle-\sum_{j=1}^{N}i\alpha_{j}+i\alpha_{j}\Big(i\alpha_{j}+i\alpha_{j}\Big)^{2}\Big(i\alpha_{j}+i\alpha_{j}\Big)^{2}}\\ &{\quad-\displaystyle\left\{\frac{i}{2}(i\alpha_{j}-i\alpha_{j})^{2}-i\alpha_{j}\Big(i\alpha_{j}+i\alpha_{j}\Big)^{2}\right\}^{2}}\\ &{=\displaystyle-\left\{\frac{i}{2}(i\alpha_{j}+i\alpha_{j})^{2}\Big(i\alpha_{j}+i\alpha_{j}\Big)^{2}-i\alpha_{j}\Big(i\alpha_{j}+i\alpha_{j}\Big)^{2}\right\}\\ &{=\displaystyle-i\alpha_{j}\Big(i\alpha_{j}+i\alpha_{j}+i\alpha_{j}\Big)^{2}\Big(i\alpha_{j}+i\alpha_{j}+i\alpha_{j}\Big)^{2}-i\alpha_{j}\ln(i\alpha_{j})-i\alpha_{j}^{2}\Big(i\alpha_{j}-i\alpha_{j}\Big)^{2}}\\ &{=\displaystyle-\frac{i}{2}\Big(i\alpha 
$$  

Thus, we see that the variance of a weighted sum of random variables is a weighted average of the variances of the individual random variables plus twice the weighted covariance. This result is commonly used in portfolio analysis because it captures the collective volatility of correlated variables.  

Now let us find the covariance between the weighted variables, that is, $\operatorname{cov}(a x,b y)$ :  

$$
{\begin{array}{l}{\displaystyle\operatorname{cov}(a x,b y)=\sum_{i=1}^{n}\left[a x_{i}-E(a x)\right]\left[b y_{i}-E(b y)\right]{\dot{p}}_{i}}\\ {=\sum_{i=1}^{n}\left[a x_{i}-a E(x)\right]\left[b y_{i}-b E(y)\right]{\dot{p}}_{i}}\\ {=\sum_{i=1}^{n}\left[a x_{i}b y_{i}-a x_{i}b E(y)-a E(x)b y_{i}+a b E(x)E(y)\right]{\dot{p}}_{i}}\end{array}}
$$  

$$
\begin{array}{l}{{\displaystyle\ =a b\sum_{i=1}^{n}x_{i}y_{i}{\boldsymbol{p}}_{i}-a b E(y)\sum_{i=1}^{n}x_{i}{\boldsymbol{p}}_{i}-a b E(x)\sum_{i=1}^{n}y_{i}{\boldsymbol{p}}_{i}+a b E(x)E(y)\sum_{i=1}^{n}p_{i}}}\\ {{\displaystyle\ =a b E(x y)-a b E(y)E(x)-a b E(x)E(y)+a b E(x)E(y)}}\\ {{\displaystyle\ =a b(E(x y)-E(x)E(y))}}\\ {{\displaystyle\ =a b\mathrm{cov}(x,y)}}\end{array}
$$  

We may also wish to know what happens to the covariance if we add constants to $x$ and $y$ . Let $a$ and $b$ be constants:  

$$
\begin{array}{l}{\displaystyle\mathrm{cov}(a+x,b+y)=\sum_{i=1}^{n}\big[a+x_{i}-E(a+x)\big]\big[b+y_{i}-E(b+y)\big]p_{i}}\\ {\displaystyle=\sum_{i=1}^{n}\big[a-E(a)+x_{i}-E(x)\big]\big[b-E(b)+y_{i}-E(y)\big]p_{i}}\\ {\displaystyle=\sum_{i=1}^{n}\big[a-a+x_{i}-E(x)\big]\big[b-b+y_{i}-E(y)\big]p_{i}}\\ {\displaystyle=\sum_{i=1}^{n}\big[x_{i}-E(x)\big]\big[y_{i}-E(y)\big]\ p_{i}=\mathrm{cov}(x,y).}\end{array}
$$  

Thus, quite logically, adding constants does not change the covariance between two variables.  

Finally, we might be interested in the covariance between the sum of two random variables and a third random variable. Letting the third random variable be $z,$ , we want to know $\mathrm{cov}(x+y,z)$ :  

$$
\begin{array}{l}{{\displaystyle\operatorname{cov}(x+y,z)=\sum_{i=1}^{n}\left[x_{i}+y_{i}-E(x+y)\right]\left[z_{i}-E(z)\right]\left.\right.\rho_{i}}}\\ {{\displaystyle\left.\operatorname{\rho}=\sum_{i=1}^{n}\left\{\left[x_{i}-E(x)\right]+\left[y_{i}-E(y)\right]\right\}\left[z_{i}-E(z)\right]\left.\right.\rho_{i}}}\\ {{\displaystyle\left.\left.=\sum_{i=1}^{n}\left[x_{i}-E(x)\right]\left[z_{i}-E(z)\right]\left.\right.\rho_{i}+\sum_{i=1}^{n}\left[y_{i}-E(y)\right]\left[z_{i}-E(z)\right]\left.\right.\rho_{i}}}\\ {{\displaystyle\left.\left.=\operatorname{cov}(x,z)+\operatorname{cov}(y,z).\right.}}\end{array}
$$  

Based on the data in the last example along with a portfolio composed of $50\%$ of instrument 1 and $50\%$ of instrument 2, Table 4.4 provides supplemental information to assist in the calculation of the covariance of sums. Here we let $P$ denote the return on a portfolio of $50\%$ instrument 1 and $50\%$ instrument 2.  

Based on Equation (4.4), we have the variance of the portfolio is  

$$
\operatorname{var}(P)=E{\big(}P^{2}{\big)}-[E(P)]^{2}=662.5-10^{2}=562.5.
$$  

TABLE 4.4 Illustration of Supplemental Information to Compute Covariance of Sums   


<html><body><table><tr><td>Probability (%)</td><td>P(%)</td><td>P²</td><td>R(1)*P (%)</td><td>R(2)*P</td></tr><tr><td>10</td><td>-2.5</td><td>6.25</td><td>62.5</td><td>-50</td></tr><tr><td>20</td><td>-10</td><td>100</td><td>50</td><td>150</td></tr><tr><td>40</td><td>5</td><td>25</td><td>50</td><td>0</td></tr><tr><td>20</td><td>12.5</td><td>156.25</td><td>250</td><td>62.5</td></tr><tr><td>10</td><td>77.5</td><td>6,006.25</td><td>4,262.5</td><td>7,750</td></tr><tr><td>Sum = 100%</td><td>E(P)= 10%</td><td>E(P2)= 662.5</td><td>E[R(1)*P]=512.5</td><td>E[R(2)* P] = 812.5</td></tr></table></body></html>  

Based on the properties of portfolio returns, we know  

$$
P_{i}=0.5(R_{1i})+0.5(R_{2i}).
$$  

Thus, based on Equation (4.15), we have the covariance as  

$$
\operatorname{cov}(0.5R_{1}+0.5R_{2},P)=\operatorname{cov}(0.5R_{1},P)+\operatorname{cov}(0.5R_{2},P).
$$  

From Equation (4.13), we know  

$$
\operatorname{cov}(0.5R_{1}+0.5R_{2},P)=0.5\operatorname{cov}(R_{1},P)+0.5\operatorname{cov}(R_{2},P),
$$  

$$
\mathrm{cov}(R_{1},P)=E(R_{1}P)-E(R_{1})E(P)=512.5-10(10)=412.5,
$$  

$$
\mathrm{cov}(R_{2},P)=E(R_{2}P)-E(R_{2})E(P)=812.5-10(10)=712.5.
$$  

Again, interpreting variances and covariances is difficult. From the definition of covariance, we observe the identity  

$$
\operatorname{var}(P)=\operatorname{cov}(0.5R_{1}+0.5R_{2},P)=0.5\operatorname{cov}(R_{1},P)+0.5\operatorname{cov}(R_{2},P).
$$  

If we divide both sides by the variance of the portfolio, we observe the following percentage marginal contribution to risk (%MCTR) of each instrument in the portfolio:  

$$
\begin{array}{r l}&{\%M C T R_{1}=0.5\frac{\mathrm{cov}(R_{1},P)}{\mathrm{var}(P)}=0.5\frac{412.5}{562.5}=36.67\%\ \mathrm{and}}\\ &{\%M C T R_{2}=0.5\frac{\mathrm{cov}(R_{2},P)}{\mathrm{var}(P)}=0.5\frac{712.5}{562.5}=63.33\%.}\end{array}
$$  

# 4.3 CONTINUOUS RANDOM VARIABLES  

For continuous random variables, the operations are somewhat different, but the results are conceptually identical to the discrete case. Consider a random variable $x$ with lower limit $x_{L}$ and upper limit $x_{H}$ . Its density function is $f(x)$ . Over the entire range of outcomes, the density integrates to 1:  

$$
\begin{array}{l}{{\displaystyle x_{H}}}\\ {{\displaystyle\int_{x_{L}}f(x)d x=1.}}\end{array}
$$  

Remember that the value of the density function at a point is not particularly useful. That is, for a given value of $x$ , say $x_{i}$ , the value $f(x_{i})$ is merely the height of the curve specified by the density, $f(x)$ . The integration of the density over a range gives us the probability of the event over that range. For example, if we wish to know if $x$ is less than or equal to $a$ , the integration of the density is  

$$
\operatorname*{Pr}(x\leq a)=\int_{x_{L}}^{a}f(x)d x.
$$  

This value is the cumulative probability that $x$ will be lower than $a$ . It is often called the distribution function and sometimes written as $F(a)$ . For a range of $a$ to $b$ , the probability that the $x$ falls in that range is found as follows:  

$$
\operatorname*{Pr}(a\leq x\leq b)=\int_{a}^{b}f(x)d x.
$$  

Note that this result can also be found as  

$$
\operatorname*{Pr}(a\leq x\leq b)=\int_{x_{L}}^{b}f(x)d x-\int_{x_{L}}^{a}f(x)d x.
$$  

Here we integrate from the lower limit up to $b$ and from the lower limit to $a$ . We then subtract the latter from the former.  

In the following material, we will set the lower limit to $-\infty$ and the upper limit to $+\infty$ .  

# 4.3.1 The Expectation of a Continuous Random Variable  

To obtain the expectation, we simply multiply each of the infinite values of the random variable by its density function and integrate,  

$$
E(x)=\int_{-\infty}^{\infty}x f(x)d x.
$$  

If $x$ is actually a constant $a$ , then we have the following result,  

$$
E(a)=\intop_{-\infty}^{\infty}a f(x)d x=a\intop_{-\infty}^{\infty}f(x)d x=a.
$$  

Thus, the expected value of a constant is simply the constant. Now, suppose we multiply the constant times the random variable,  

$$
E(a x)=\int_{-\infty}^{\infty}a x f(x)d x=a\int_{-\infty}^{\infty}x f(x)d x=a E(x).
$$  

And we simply pulled the constant out of the integration.  

# 4.3.2 The Variance of a Continuous Random Variable  

The variance of a continuous random variable $X$ is given as  

$$
\operatorname{var}(x)=\int_{-\infty}^{\infty}[x-E(x)]^{2}f(x)d x.
$$  

The key results for the variance are given as follows. First note that the variance itself can be restated as  

$$
\begin{array}{l}{\displaystyle\operatorname{var}(x)=\int_{-\infty}^{\infty}(x-E(x))^{2}f(x)d x=\int_{-\infty}^{\infty}\left\{x^{2}-2x E(x)-[E(x)]^{2}\right\}f(x)d x}\\ {\displaystyle\qquad\infty}\\ {\displaystyle\qquad=\int_{-\infty}^{\infty}x^{2}f(x)d x-2E(x)\int_{-\infty}^{\infty}x f(x)d x+[E(x)]^{2}\int f(x)d x}\\ {\displaystyle\qquad-\infty}\\ {\displaystyle=E\big(x^{2}\big)-2[E(x)]^{2}+[E(x)]^{2}=E\big(x^{2}\big)-[E(x)]^{2}.}\end{array}
$$  

If $x$ is a constant, we have the following:  

$$
\operatorname{var}(a)=\int_{-\infty}^{\infty}[a-E(a)]^{2}f(x)d x=\int_{-\infty}^{\infty}(a-a)^{2}f(x)d x=0.
$$  

Obviously, the variance of a constant is zero. Now suppose we multiply a constant times the random variable.  

$$
\begin{array}{l}{\displaystyle\operatorname{var}(a x)=\int_{-\infty}^{\infty}[a x-E(a x)][a x-E(a x)]f(x)d x}\\ {\displaystyle\operatorname{\rhose}}\\ {\displaystyle=\int_{-\infty}^{\infty}a[x-E(x)]a[x-E(x)]f(x)d x=a^{2}\int_{-\infty}^{\infty}[x-E(x)]^{2}f(x)d x=a^{2}\mathrm{var}(x).}\end{array}
$$  

So, the constant comes out of the variance as a square. Suppose we add a constant to the random variable.  

$$
\begin{array}{l}{{\displaystyle\mathrm{var}(a+x)=\int_{-\infty}^{\infty}\left[a+x-E(a+x)\right]^{2}f(x)d x}\ ~}\\ {{\displaystyle\mathrm{\Omega}_{-\infty}^{\infty}}}\\ {{\displaystyle=\int_{-\infty}^{\infty}\left[a-E(a)+x-E(x)\right]^{2}f(x)d x=\int_{-\infty}^{\infty}\left[a-a+x-E(x)\right]^{2}f(x)d x}\ ~}\\ {{\displaystyle\mathrm{\Omega}_{-\infty}^{\infty}}}\\ {{\displaystyle=\int_{-\infty}^{\infty}\left[x-E(x)\right]^{2}f(x)d x=\mathrm{var}(x)}.}\end{array}
$$  

Adding a constant clearly has no effect on the variance.  

# 4.3.3 The Covariance of Continuous Random Variables  

The covariance of $x$ and $y$ is  

$$
\operatorname{cov}(x,y)=\int_{-\infty}^{\infty}[x-E(x)][y-E(y)]f(x y)d x y.
$$  

In the previous equation, $f(x y)$ is the joint density of $x$ and $y$ , which gives the probability of outcomes of both $x$ and $y$ occurring. The single integral in the covariance equation means to integrate over all joint outcomes. At this point it will be useful to introduce a substitution for the integral and the density function.  

$$
\int_{-\infty}^{\infty}f(x y)d x y=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(y|x)f(x)d x d y=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(x|y)f(y)d y d x.
$$  

Here we substitute the product of the conditional and marginal densities for the joint density. When we do so, we must have an integral and a differential for each density. Also note that any time we see any of these expressions with no other terms, it equals 1.0. In other words,  

$$
\int_{-\infty}^{\infty}f(y|x)d y=\int_{-\infty}^{\infty}f(x|y)d x=1.
$$  

Now our covariance can be written as  

$$
\begin{array}{r l}&{\quad\quad\sum_{i=1}^{N}\left\{\left[(\gamma_{i}+\gamma_{i})(\gamma_{i})-\gamma_{i}(k)+k(\gamma_{i}+\gamma_{i})\right](\zeta_{i})\right\}k_{i,j}}\\ &{\quad=\displaystyle\int_{-\infty}^{\infty}\left(\gamma_{i}(\gamma_{i})d\gamma_{i}-k(\gamma_{i})\right)^{2}\frac{1}{\int_{-\infty}^{\infty}\left(\gamma_{i}(\gamma_{i})d\gamma_{i}-k(\gamma_{i})\right)^{2}d\gamma_{i}}\qquad\mathrm{~for~ady~}}\\ &{\quad=\displaystyle\int_{-\infty}^{\infty}\left(\gamma_{i}(\gamma_{i})d\gamma_{i}-k(\gamma_{i})\right)^{2}\frac{1}{\int_{-\infty}^{\infty}\left(\gamma_{i}(\gamma_{i})d\gamma_{i}-k(\gamma_{i})\right)^{2}d\gamma_{i}}\qquad\mathrm{~for~ady~}}\\ &{\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad}\\ &&{\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad}\\ &&{\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad}\\ &{\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad}\\ &{\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad}\\ &{\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad
$$  

So we have  

$$
\operatorname{cov}(x,y)=E(x y)-E(x)E(y).
$$  

Covariances with constants are zero as indicated by the following:  

$$
\operatorname{cov}(a,x)=\int_{-\infty}^{\infty}[a-E(a)][x-E(x)]f(x y)d x y=\int_{-\infty}^{\infty}(a-a)[x-E(x)]f(x y)d x y=0.
$$  

Next we show the important result that the variance of a weighted combination of random variables is a weighted combination of their variances and all possible pairwise covariances.  

$$
\begin{array}{r l}{\operatorname{conta}\ \operatorname{i}\beta\ \operatorname{i}\beta\ }&{\overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ }\\ &{=\ \overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \underset{{\mathrm{\tiny\beta\geq1}}}{\longrightarrow}\ \left(\ w-\ \overset{1}{\underset{\mathrm{\tiny\beta\geq1}}{\ Z}}\ \right)+\ \overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ }\\ &{=\ \overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \left(\ u^{\mathrm{\textnormal{\texttt{\beta}}}}\ \ \mathscr{R}\ \xi\right)\ \overset{\ }{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \ \underset{{\mathrm{\tiny\beta\geq1}}}{\longrightarrow}\ \left(\ y-\overset{1}{\underset{\mathrm{\tiny\beta\geq1}}{\ Z}}\ \right)\ \underset{{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \left(y\right)\ }\\ &{=\overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \left(\underset{{\mathrm{\tiny\beta\geq1}}}{\overset{,}{\longrightarrow}}\ \ \overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \right)+\ 2\ \mathrm{\normalfont\beta}\ (x-\overset{\beta\leq1}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \left\Vert\nabla-\overset{\beta\geq1}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\right\Vert\ \mathrm{\tiny\beta\geq}\ }\\ &{=\:\overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \overset{,}{\underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}}\ \underset{\mathrm{\tiny\beta\geq1}}{\longrightarrow}\ \left(\mathrm{\texttt{\beta}}\right)\ \underset{\mathrm{\tiny\geq1}}{
$$  

Recall that previously we substituted the product of the conditional and marginal densities for the joint density and split the single integral into the product of two integrals. Using these tricks, we obtain $a^{2}\mathrm{var}(x)$ for the first term, $b^{2}\mathrm{var}(y)$ for the second and $2a b\mathrm{cov}(x,y)$ . Thus,  

$$
\operatorname{var}(a x+b y)=a^{2}\operatorname{var}(x)+b^{2}\operatorname{var}(y)+2a b\operatorname{cov}(x,y).
$$  

The covariance between two weighted random variables is simply their covariance times the product of their weights:  

$$
\operatorname{cov}(a x,b y)=\int_{-\infty}^{\infty}[a x-E(a x)][b y-E(b y)]f(x y)d x y
$$  

$$
\begin{array}{l}{{\displaystyle\ }}\\ {{\displaystyle\ }}\\ {{\displaystyle\ }}\\ {{\displaystyle\ }}\\ {{\displaystyle\ }}\\ {{\displaystyle\ }}\\ {{\displaystyle\ }}\end{array}\overset{\infty}{\underset{-\infty}{\int}}\{{\alpha[x-E(x)]b[y-E(y)]\nmid f(x y)d x y\ }}\\ {{\displaystyle\ }}\\ {{\displaystyle}}\\ {{\displaystyle\ }}\\ {{\displaystyle}}\\ {{\displaystyle}}\end{array}
$$  

This result is widely used in portfolio analysis, a point we made in the discrete case.  

The covariance between the sum of a constant and random variables times another sum of a constant and a random variable is simply the covariance of the random variables:  

$$
\begin{array}{r l}&{\mathrm{cove}(a+x,b+y)=\displaystyle\int_{-\infty}^{\infty}[a+x-E(a+x)][b+y-E(b+y)]f(x y)d x y}\\ &{\mathrm{~}\longrightarrow}\\ &{\mathrm{~}=}\\ &{\mathrm{~}=\displaystyle\int_{-\infty}^{\infty}[a-E(a)+x-E(x)][b-E(b)+y-E(y)]f(x y)d x y}\\ &{\mathrm{~}\longrightarrow}\\ &{\mathrm{~}=\displaystyle\int_{-\infty}^{\infty}[a-a+x-E(x)][b-b+y-E(y)]f(x y)d x y}\\ &{\mathrm{~}=\displaystyle\int_{-\infty}^{\infty}[a-E(x)][y-E(y)]f(x y)d x y=\mathrm{cove}(x,y).}\\ &{\mathrm{~}=\displaystyle\int_{-\infty}^{\infty}[x-E(x)][y-E(y)]f(x y)d x y=\mathrm{cove}(x,y).}\end{array}
$$  

One of the more complex results is the covariance between the sum of two random variables and a third random variable:  

$$
\begin{array}{l}{\displaystyle\mathrm{cov}(x+y,z)=\int_{-\infty}^{\infty}[x+y-E(x+y)][z-E(z)]f(x y z)d x y z}\\ {\displaystyle-\infty}\\ {\displaystyle\qquad\mathrm{\simeq}}\\ {\displaystyle=\int_{-\infty}^{\infty}[x-E(x)+y-E(y)][z-E(z)]f(x y z)d x y z}\\ {\displaystyle\qquad\mathrm{\to}}\\ {\displaystyle=\int_{-\infty}^{\infty}[x-x E(z)-E(x)z+E(x)E(z)+y z-y E(z)-E(y)z+E(y)E(z)]}\\ {\displaystyle\qquad=\int_{-\infty}^{\infty}[x z-x E(z)-E(x)z+E(x)E(z)+y z-y E(z)-E(y)z+E(y)E(z)]}\end{array}
$$  

$$
\begin{array}{c}{{\displaystyle+\displaystyle\int_{-\infty}^{\infty}y z f(x y z)d x y z-E(z)\displaystyle\int_{-\infty}^{\infty}y f(x y z)d x y z}}\\ {{\displaystyle-E(y)\displaystyle\int_{-\infty}^{\infty}z f(x y z)d x y z+E(y)E(z)\displaystyle\int_{-\infty}^{\infty}f(x y z)d x y z.}}\end{array}
$$  

As we did previously when working with only two random variables, we need to convert the joint density into the product of the marginal and conditional densities. Now, however, we have three variables. We use the following relationships:  

$$
{\begin{array}{r l}&{f(x y z)=f(y|x z)f(x z)=f(x|y z)f(y z)=f(z|x y)f(x y)}\\ &{\qquad=f(y z|x)f(x)=f(x z|y)f(y)=f(x y|z)f(z).}\end{array}}
$$  

As we have previously noted, the integral of any density function over its entire domain is 1.0. Thus,  

$$
\int_{-\infty}^{\infty}f(y z|x)d y z=\int_{-\infty}^{\infty}f(x z|y)d x z=\int_{-\infty}^{\infty}f(x y|z)d x y=1.0.
$$  

Of the eight expressions for the covariance, the first is, therefore,  

$$
\begin{array}{c}{{\displaystyle\displaystyle\int_{-\infty}^{\infty}x z f(x y z)d x y z=\displaystyle\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}x z f(y|x z)f(x z)d y d x z\ ~}}\\ {{\displaystyle=\displaystyle\int_{-\infty}^{\infty}f(y|x z)d y\displaystyle\int_{-\infty}^{\infty}x z f(x z)d x z=E(x z).}}\end{array}
$$  

The second is  

$$
-E(z)\intop_{-\infty}^{\infty}x f(x y z)d x y z=-E(z)\intop_{-\infty}^{\infty}f(y z|x)d y z\intop_{-\infty}^{\infty}x f(x)d x
$$  

The third is  

$$
-E(x)\intop_{-\infty}^{\infty}z f(x y z)d x y z=-E(x)\intop_{-\infty}^{\infty}f(x y|z)d x y\intop_{-\infty}^{\infty}z f(z)d z
$$  

The fourth expression is clearly $E(x)E(z)$ . The fifth expression is  

$$
\begin{array}{l}{{\displaystyle+\int_{-\infty}^{\infty}y z f(x y z)d x y z=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}y z f(z|y z)f(x y z)d x d y z\ ~}}\\ {{\displaystyle~\qquad\infty~}}\\ {{\displaystyle=\int_{-\infty}^{\infty}f(x|y z)d x\int_{-\infty}^{\infty}y z f(y z)d y z=E(y z).}}\end{array}
$$  

The sixth expression is  

$$
-E(z)\intop_{-\infty}^{\infty}y f(x y z)d x y z=-E(z)\intop_{-\infty}^{\infty}f(x z|y)d x z\intop_{-\infty}^{\infty}y f(y)d y
$$  

The seventh expression is  

$$
-E(y)\intop_{-\infty}^{\infty}z f(x y z)d x y z=-E(y)\intop_{-\infty}^{\infty}f(x y|z)d x y\intop_{-\infty}^{\infty}z f(z)d z
$$  

The eighth expression is clearly $E(y)E(z)$ . Thus, overall our covariance is  

$$
\begin{array}{r l}&{E(x z)-E(z)E(x)-E(x)E(z)+E(x)E(z)+E(y z)-E(z)E(y)-E(y)E(z)+E(y)E(z)}\\ &{\qquad=E(x z)-E(x)E(y)+E(y z)-E(y)E(z)=\mathrm{Cov}(x,z)+\mathrm{Cov}(y,z).}\end{array}
$$  

# 4.4 SOME GENERAL RESULTS IN PROBABILITY THEORY  

In the following subsections, we look at some of the general results from probability theory that are helpful in option theory. By the term general, we mean that these results are not dependent on any particular probability distribution. When a specific probability distribution is known, usually a stronger statement can be made.  

# 4.4.1 The Central Limit Theorem  

The central limit theorem is a powerful statement that tells us that sum or average of independent samples drawn from any given probability distribution becomes normally distributed in the limit, that is, as the sample size approaches infinity. Thus, provided the sample size is large enough, the central limit theorem enables us to use the rules associated with normal probability theory when drawing inferences about sample means. How large the sample size must be to rely on the central limit theorem is not known, but a common rule of thumb has always been at least 30.  

# 4.4.2 Chebyshev’s Inequality  

Chebyshev’s inequality, sometimes called Chebyshev’s theorem, enables us to make a sometimes useful statement about the probability of a sample value deviating from the population mean. More precisely, let $x$ be a random variable with mean $\mu$ and variance $\sigma^{2}$ , which can come from any probability distribution. For any real number $z>0$ ,  

$$
\operatorname*{Pr}(|x-\mu|\geq z\sigma)\leq{\frac{1}{z^{2}}},{\mathrm{assuming~}}z\geq1.
$$  

Thus, Chebyshev’s theorem gives an upper bound on how much the observed value deviates from the mean in terms of an arbitrary value $z$ . For example, let $x$ be the average height in inches of a randomly drawn male university student. Let $\mu=70$ and $\sigma=2$ . For $z$ equals various values, we have the following results:  

<html><body><table><tr><td></td><td></td><td>Maximum Pr(|x - μl ≥ zo)</td></tr><tr><td>５</td><td>10</td><td>0.04</td></tr><tr><td>4</td><td>8</td><td>0.0625</td></tr><tr><td>3</td><td>6</td><td>0.1111</td></tr></table></body></html>  

In other words, the probability that a sample value will deviate from the mean by more than 10 inches is less than 0.04. Thus, the probability that a student will be more than 80 inches (70 plus 10 inches) is less than $4\%$ . The probability that a student will be more than 78 inches (70 plus eight) is less than $6.25\%$ . The probability that a student will be more than 76 inches (70 plus six) is less than $11.11\%$ . These rules hold for any distribution, though knowing the exact distribution usually enables one to make more precise statements.11  

# 4.4.3 The Law of Large Numbers  

The law of large numbers provides information about the accuracy with which a sample mean approximates a population mean. Basically, it says that the probability that the difference between the sample mean and the population mean is greater than an arbitrarily chosen small value is zero because the sample size goes to infinity. This law holds as long as the sample consists of independently selected random variables from the same distribution. The law of large numbers more or less says that if we take sufficiently large samples, our sample mean estimate converges to the population mean.  

# 4.4.4 The Law of Iterated Expectations (the Tower Law)  

The law of iterated expectations, sometimes called the Tower law, is used to specify the expected value assessed at a given time $t$ in terms of an expected value at another time $t+i,$ , which is taken as an expected value at a later time $t+j$ . In other words, we are taking the expectation of an expectation. For example, suppose we are at time $t+i$ and are calculating the expected value at time $t+j$ . We might denote this as $E_{t+\mathrm{i}}(x_{t+j})$ , which simply means that using the information at time $t+i,$ , we assess the expected value of $x$ to occur at a later time $t+j$ . Now step back to time $t$ and try to assess the expected value at time $t+i$ . In other words, we want $E_{t}[E_{t+i}(x_{t+j})]$ . The law of iterated expectations simply says  

$$
E_{t}\left[E_{t+i}\left(\boldsymbol{x}_{t+j}\right)\right]=E_{t}\left(\boldsymbol{x}_{t+j}\right).
$$  

In other words, the expectation is iterated from the later time to the earlier time.  

# 4.4.5 The Law of Total Probability  

The law of total probability is a simple statement about the conditional probabilities and marginal probabilities. Specifically let $y$ and $x$ be random variables where $x$ is bifurcated into values greater than $b$ and less than or equal to $b$ . Then the law of total probability states that  

$$
\operatorname*{Pr}(y)=\operatorname*{Pr}(y|x>b)\operatorname*{Pr}(x>b)+\operatorname*{Pr}(y|x\leq b)\operatorname*{Pr}(x\leq b).
$$  

The statement $\mathrm{Pr}(y)$ is simply any specification of $y$ , such as $\operatorname*{Pr}(y>k)$ or $\operatorname*{Pr}(a\leq y\leq k)$ . So, the probability of any event associated with $y$ can be broken down into the probability of that event for $y$ , conditional on an event occurring for $x$ , and the probability of that event for $y$ , conditional on that event not occurring for $x$ .  

# 4.5 TECHNICAL INTRODUCTION TO COMMON PROBABILITY DISTRIBUTIONS USED IN FINANCE  

Numerous financial challenges are addressed with the use of common probability distributions. As noted previously, probability distributions are either discrete or continuous.  

Discrete distributions have a countable set of possible outcomes, such as flipping a coin, where each outcome has a nonnegative chance of occurring and the sum of the probability of all outcomes is equal to one. More formally, for a discrete probability function, $p_{X}(a)$ , the following properties hold for a specific value, $x$ :  

$\operatorname*{Pr}(x=a)=p_{X}(a)$ . [Read: The probability that $X$ is a specific value, $x=a$ , is $p_{X}(a)$ .] $p_{X}(x)\geq0\forall x$ . [Read: The probability is nonnegative for all possible outcomes $x$ .] $N$ $\sum_{j=1}^{}{p_{X}(x_{j})}=1$ . [Read: The sum of the probabilities over all possible values of $x$ is 1.]  

Discrete probability distributions covered here include the binomial and Poisson distributions.  

Continuous distributions have an infinitesimal probability of achieving any particular outcome, whereas the likelihood of achieving a value within a defined range of outcomes is measurable. Finally, the integral over the range of all outcomes is equal to one. More formally, for a continuous probability function, $p_{c}(x)$ , the following properties hold:  

$\scriptstyle x=b$ $\textstyle\operatorname*{Pr}(a\leq x\leq b)=\int_{x=a}p_{c}(x)d x$ . [Read: The probability that $x$ takes a value in a range of a through b.]  

$\operatorname*{Pr}(a\leq x\leq b)\geq0\forall a\leq b$ . [Read: The probability is nonnegative for all possible ranges of $a$ through $b$ .]  

$\scriptstyle x=+\infty$ $\begin{array}{r}{\operatorname*{Pr}(-\infty\leq x\leq+\infty)=\int_{\stackrel{}{x=-\infty}}{p_{c}(x)d x}=1.}\end{array}$ [Read: The integral over all possible values of −∞ $x$ is 1.]  

Continuous probability distributions covered here include the normal and lognormal distributions.  

# 4.5.1 Binomial Distribution  

The binomial distribution is used to model situations involving two random outcomes, such as a coin toss. Thus, this discrete probability distribution is binary, derived from the Latin root bi meaning “two.” The -nomial extension is derived from Latin, meaning “name.” Thus, the binomial distribution is based on two names, such as true and false, heads or tails, 1 or 0, up or down, and so forth. We often select one of the names to be success and the other name to be failure. The binomial distribution is a two-parameter distribution, with parameters $n$ and $p$ . The parameter $n$ denotes the number of independent events. In finance, $n$ is often the number of time steps, such as 252 for number of trading days in one particular year. The parameter $p$ denotes the probability of observing a success, such as true, heads, 1, up, and so on. In finance, $\boldsymbol{p}$ is often the probability that a financial instrument will go up over the next time step.  

The possible outcomes from a binomial distribution range from 0 to $n$ , such as the number of observed up events over $n$ days, where each event has probability $\boldsymbol{p}$ of success. The binomial distribution is therefore denoted $B(n,p)$ . If $x$ is said to have a binomial distribution, then the probability mass function can be expressed as12  

$$
b(i,n,p)=\operatorname*{Pr}(i;n,p)=\operatorname*{Pr}(x=i)={\binom{n}{i}}p^{i}(1-p)^{n-i},
$$  

for all $i=0,1,2,\ldots,n$ , where the combination of $n$ events with $i$ successes is defined as  

$$
{\binom{n}{i}}={\frac{n!}{i!(n-i)!}},
$$  

and $n$ ! denotes factorial or $n(n-1)(n-2)\dots(2)1$ . The combination of $n$ events with $i$ successes identifies the number of different ways of observing $i$ successes over $n$ events.  

The cumulative distribution function (CDF) can be expressed as  

$$
B({\boldsymbol{j}};n,p)=\operatorname*{Pr}(x\leq j)=\sum_{i=0}^{j}{\binom{n}{i}}p^{i}(1-p)^{n-i}.
$$  

The mean is $n(\boldsymbol{p})$ and the variance is $n(p)(1-p)$ . We will explore this distribution in greater detail in Chapter 7.  

# 4.5.2 Poisson Distribution  

The Poisson distribution is used to model countable outcomes, such as the number of companies registered in a particular year. This distribution is named after Siméon Denis Poisson (1781–1840), although Abraham de Moivre (1711) appears to be the first to develop it. The Poisson is also a discrete probability distribution. In finance, the Poisson distribution is used when one is interested in the number of times a particular event happens during a specified time span. This distribution assumes that the occurrence of one event does not affect the likelihood of observing another event as well as assumes the average number of events is constant.  

The possible outcomes from a Poisson distribution are the integers from 0 to $\infty$ , such as the number of bankruptcies observed in a given year. The Poisson distribution requires only one parameter, the average number of observed outcomes during an interval of time, denoted $\lambda$ . Thus, the Poisson distribution is denoted $P D(\lambda)$ . If $\mathbf{x}$ is said to have a Poisson distribution, then the probability mass function can be expressed as  

$$
p m(i,\lambda)=\mathrm{Pr}(i;\lambda)=\mathrm{Pr}(x=i)={\frac{\lambda^{i}e^{-\lambda}}{i!}},
$$  

for all $i=0,1,2,\ldots,\infty$ .  

The cumulative distribution function (CDF) can be expressed as  

$$
P M(j;\lambda)=\operatorname*{Pr}(x\leq j)=e^{-\lambda}\sum_{i=0}^{j}\frac{\lambda^{i}}{i!}.
$$  

The mean and variance are both $\lambda$ .  

# 4.5.3 Normal Distribution  

The normal distribution is one of the most well-known of all distributions and is widely used in finance; hence, we devote the entire Chapter 5 to it. We provide a brief technical introduction here. The range of a variable, $x.$ , that follows a normal distribution is $-\infty<$ $x<+\infty$ . It is a symmetric two-parameter distribution typically identified with the mean, $\mu$ , and standard deviation, $\sigma$ . The range for the mean is therefore $-\infty<\mu<+\infty$ and the range for the standard deviation is $\sigma>0$ . If $x$ is said to have a normal distribution, then the probability density function (PDF) can be expressed as  

$$
n(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}.
$$  

The PDF is the derivative of the cumulative distribution function. The CDF, assuming $-\infty<a<+\infty$ , can be expressed as  

$$
N(a)=\int_{x=-\infty}^{a}n(x)d x=\int_{x=-\infty}^{a}\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}d x.
$$  

The CDF range is $0\leq N(a)\leq1$ and provides the probability that an outcome is less than or equal to $a$ . The PDF is simply the first derivative of the CDF.  

We briefly identify several location statistics related to the normal distribution. The median is defined as the $\widehat{\boldsymbol{x}}$ such that $\begin{array}{r}{\frac{\widehat{x}}{x}\qquad\hfill+\infty}\\ {-\infty}\end{array}$ for any PDF $f(x)$ . For the normal distribution, the median is equal to $\mu$ (a constant). The mode is defined as the $\widehat{\boldsymbol{x}}$ such that $\begin{array}{r}{\frac{d f(x)}{d x}=0}\end{array}$ and $\begin{array}{r}{\frac{d^{2}f(x)}{{d x}^{2}}<0}\end{array}$ for any PDF. For the normal distribution, the mode is also equal to $\mu$ .  

With statistical distributions, the $n^{t h}$ noncentral moment is defined as ${\mu_{n}}^{\prime}(x)=E(x^{n})$ and the $n^{t h}$ central moment is defined as $\mu_{n}(x)=E[(x-\mu)^{n}]$ . The first noncentral moment is the mean or  

$$
M e a n=\mu_{1}{}^{\prime}(x)=\mu.
$$  

The mean is a constant and the first central moment is zero or $\mu_{1}(x)=0$ . The second central moment is the variance or  

$$
V a r i a n c e=\mathrm{var}(x)=\mu_{2}(x)=\sigma^{2}.
$$  

The variance is a constant and the second noncentral or raw moment is $\mu_{2}^{\prime}(x)=\mu^{2}+\sigma^{2}$ . The third central moment is the skewness or  

$$
S k e w n e s s=\mu_{3}(x)=0.
$$  

Skewness of zero implies a symmetric distribution. The third noncentral moment is $\mu_{3}^{\:\prime}(x)=\mu^{3}+3\mu\sigma^{2}$ . The fourth central moment is the kurtosis or  

$$
K u r t o s i s=\mu_{4}(x)=3\sigma^{4}.
$$  

Note that the fourth moment is positive and a function of variance. The fourth noncentral or raw moment is $\mu_{4}{}^{\prime}(x)=\mu^{4}+6\mu^{2}\sigma^{2}+3\sigma^{4}$ . Kurtosis is often normalized by the variance squared. Thus, the kurtosis of the normal distribution is 3 and is known as mesokurtic.  

Figures $4.1\mathrm{a}\mathrm{-}\mathrm{h}$ illustrate both the probability density function as well as the cumulative distribution function of the normal distribution applied to some financial instrument price. The following series of figures illustrates the effect of increases in the standard deviation because different financial instruments have different levels of standard deviation. One common challenge is misinterpreting high standard deviation instruments, particularly when applying the lognormal distribution, which we cover next.  

We see from these graphs that with the normal distribution, there is a nonzero probability that the underlying instrument value can fall below zero. Financial derivative instruments can often themselves take on negative values. Negative value is very common for symmetric derivatives such as forwards, futures, and swaps as well as short positions in asymmetric derivatives such as options, swaptions, caps, and floors. For example, the short position for any option cannot be positive as it can only result in a future liability. A call option price may be 14, but to the writer it is a liability of 14 and hence has a negative value. Because common stock has limited liability, we expect the stock price to remain nonnegative. Limited liability can be viewed as a company obtaining a long put option contract with a zero strike price. Clearly, company assets can obtain a negative value, such as is the case with environmental damage or company products found to be carcinogenic. Again, more extensive details on the normal distribution are given in Chapter 5. To address some of these challenges, many financial models are built on the lognormal distribution.  

![](fa4716bfd835c3461d19bb0c2dfbcc8dc165b150ccd930a2e8027cda873bc777.jpg)  
FIGURE 4.1a Normal Probability Density Function $\%=0\%$ , $\sigma=30\%$ )  

![](fa61510bd1c1ae3b86015acdf4b4fe6403ef272e060fed17e9df3d70cc10235a.jpg)  
FIGURE 4.1b Normal Cumulative Distribution Function $\%=0\%$ , $\sigma=30\%$ )  

![](3155bb533e9d0c08e7459648cc7132c60c8c88f4f574b7394bed94003044d60c.jpg)  
FIGURE 4.1c Normal Probability Density Function $\%=0\%$ , $\sigma=80\%$ )  

![](1487f11bf512853ff800c4e4d4fbd7c6be3e2bc98395261cbc060786bf6b7389.jpg)  
FIGURE 4.1d Normal Cumulative Distribution Function $\dot{\mu}=0\%$ , $\sigma=80\%$ )  

![](26247675a0fa5706d2d78358c25347aa4668fa98f1d02a70302057f47c95354a.jpg)  
FIGURE 4.1e Normal Probability Density Function $\%=0\%$ , $\sigma=130\%$ )  

![](f4d03a4b346e011471f7dff2f6cefe932d262840ad32613b53026beeeb1b904f.jpg)  
FIGURE 4.1f Normal Cumulative Distribution Function $\dot{\boldsymbol{\mu}}=0\%$ , $\sigma=130\%$ )  

![](8f58c8daf7274d88da34a9e6ae84e6d67e573416c64de22ea5ac1ab6f489dc48.jpg)  
FIGURE 4.1g Normal Probability Density Function $\%=0\%$ , $\sigma=800\%$ )  

![](6909d9cbe996cf2e3d26a52f195aae8c9c1570486fd8dca5ad990dcbc93070d3.jpg)  
FIGURE 4.1h Normal Cumulative Distribution Function $\dot{\mu}=0\%$ , $\sigma=800\%$ )  

# 4.5.4 Lognormal Distribution  

The lognormal distribution is related to the normal distribution in the following way: If $y=\ln(x)$ is normally distributed, $x$ is said to be lognormally distributed. It is an asymmetric two-parameter distribution again identified with the implied normal distribution’s mean, $\mu$ , and standard deviation, $\sigma$ . The range for the mean is $-\infty<\mu<+\infty$ and the range for the standard deviation is $\sigma>0$ . The range of a variable, $x$ , that follows a lognormal distribution is $0<x<+\infty$ . Note that zero is not included. If $x$ is said to have a lognormal distribution, then the PDF can be expressed as (where the Greek lambda, $\lambda$ , here is not to be confused with the Poisson distribution parameter introduced previously):  

$$
\lambda(x)=\frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{[\ln(x)-\mu]^{2}}{2\sigma^{2}}}.
$$  

The CDF, assuming $0<a<+\infty$ , can be expressed as  

$$
\Lambda(a)=\int_{x=-\infty}^{a}\lambda(x)d x=\int_{x=-\infty}^{a}\frac{1}{x\sigma\sqrt{2\pi}}e^{-\frac{[\ln(x)-\mu]^{2}}{2\sigma^{2}}}d x.
$$  

Similar to the normal distribution, the CDF range is $0\leq\Lambda(a)\leq1$ and provides the probability that an outcome is less than or equal to $a$ .  

We briefly identify several location statistics related to the lognormal distribution. The median for the lognormal distribution is  

$$
M e d i a n=e^{\mu}.
$$  

The mode is  

$$
M o d e=e^{\mu-\sigma^{2}}.
$$  

Recall the first noncentral moment is the mean and can be expressed as  

$$
M e a n=E(x)={\mu_{1}}^{\prime}(x)=e^{\mu+{\frac{\sigma^{2}}{2}}}{\sqrt{b^{2}-4a c}}.
$$  

The mean is a constant and the first central moment is zero or $\mu_{1}(x)=0.$ . The second central moment is the variance or  

$$
\operatorname{var}(x)=\mu_{2}(x)=e^{2\mu+\sigma^{2}}(e^{\sigma^{2}}-1).
$$  

The variance is a constant and the second noncentral moment is $\mu_{2}^{\prime}(x)=\mathrm{e}^{2(\mu+\sigma^{2})}$ . The third central moment is the skewness or  

$$
S k e w(x)=\mu_{3}(x)=e^{3\mu+{\frac{3\sigma^{2}}{2}}}(e^{\sigma^{2}}-1)^{2}(e^{\sigma^{2}}+2).
$$  

The skewness of the lognormal distribution is positive, which implies an asymmetric distribution, specifically positive or right skewed. The third noncentral moment is $ {\mu_{3}}^{\prime}(x)=$ $e^{3\left(\mu+{\frac{3}{2}}\sigma^{2}\right)}$ ). Similar to variance and covariance, skewness is difficult to interpret. Thus, skewness is often normalized in the following manner:  

$$
N S k e w\equiv\frac{\mu_{3}^{\prime}}{V a r^{2/3}}=(e^{\sigma^{2}}-1)^{2}(e^{\sigma^{2}}+2).
$$  

The normalized skewness is an exponentially increasing function of the normal distribution variance. Symmetrical distributions, similar to the normal distribution, will have $N S k e w=0$ . If $N S k e w>0$ as is the case for the lognormal distribution, then the mean is greater than the median and the median is greater than the mode. If $N S k e w<0$ as is common with empirical finance distributions, then we have mean $\prec$ median $\prec$ mode. There are, however, some rare exceptions to this pattern; see Stuart and Ord (1987: 107).  

The fourth central moment is the kurtosis or  

$$
K u r t o s i s(x)=\mu_{4}(x)=e^{3\mu+\frac{3\sigma^{2}}{2}}(e^{\sigma^{2}}-1)^{2}(e^{4\sigma^{2}}+2e^{3\sigma^{2}}+3e^{2\sigma^{2}}-3).
$$  

Note that the fourth moment is positive and a function of the variance. The fourth noncentral moment is $\mu_{4}{}^{\prime}(x)=e^{4(\mu+2\sigma^{2})}$ . Kurtosis is often normalized by the variance squared. Thus, the normalized kurtosis of the lognormal distribution is  

$$
N K u r t=\frac{\mu_{4}}{{V a r}^{2}}=(e^{\sigma^{2}}-1)(e^{3\sigma^{2}}+3e^{2\sigma^{2}}+6e^{\sigma^{2}}+6).
$$  

The kurtosis is an exponentially increasing function of the normal distribution variance. Recall that the normal distribution is mesokurtic, that is, $N K u r t=3$ . Because variance is always positive, for the lognormal distribution, $N K u r t>3$ and is known as leptokurtic. As an aside, $N K u r t<3$ is called platykurtic.  

Before illustrating graphically the lognormal distribution, we establish the link between the normal distribution parameters, $\mu$ and $\sigma$ , and the expected value and standard deviation of the lognormally distributed variable. We make this link by illustrating with asset prices. Recall if variable $x$ is distributed normal, denoted $x\sim N(\mu,\sigma)$ , then variable $y$ defined as $y=\exp(x)$ is distributed lognormal, denoted $x\sim\Lambda(\mu,\sigma)$ . In the context of rates of return, suppose $S_{T}=S_{t}e^{R(T-t)}$ . If $R\sim N(\mu,\sigma)$ , then we know  

$$
S_{T}\sim\Lambda\left[\ln(S_{t})+\mu(T-t),\sigma\sqrt{T-t}\right].
$$  

Thus, $E(S_{T})=S_{0}e^{\left(\mu+\frac{\sigma^{2}}{2}\right)(T-t)},\qquad\mathrm{var}(S_{T})=S_{0}^{2}\Big[e^{2(\mu+\sigma^{2})(T-t)}-e^{(2\mu+\sigma^{2})(T-t)}\Big],$ and $S D(S_{T})=\sqrt{S_{0}^{2}\left[e^{2(\mu+\sigma^{2})(T-t)}-e^{(2\mu+\sigma^{2})(T-t)}\right]}$ . It is important to note that in finance the normal distribution parameters are typically expressed in percentage ( $\mu$ and $\sigma$ here), whereas the lognormal mean and standard deviation $(\operatorname{E}(S_{T})$ and ${\bf v a r}(S_{T})$ here) are typically expressed in currency units. Remember both distributions are two-parameter distributions. Often with both the lognormal and normal distributions, these two parameters are assumed to be the normal distribution’s mean $(\mu)$ and standard deviation $(\sigma)$ . One could just as easily represent the lognormal distribution parameters with the lognormal distribution’s mean, $E(S_{T})$ , and standard deviation, $S D(S_{T})$ . For example, suppose the normal distribution’s mean is $\mu=9.5\%$ and the standard deviation is $\sigma=38\%$ . If an asset price is trading at 100 and suppose we have a one-year horizon, we can compute $E(S_{T})=118.20$ and $S D(S_{T})=46.59$ based on the previous equations. Thus, the mean and standard deviation of the lognormally distributed asset price is expressed in dollars or other currency units.  

Alternatively, the normal distribution parameters can be expressed as a function of the lognormal distribution parameters or  

$$
\begin{array}{r l}&{\mu=\frac{\displaystyle\ln\left[\frac{E(S_{T})}{S_{0}}\right]}{T-t}-\frac{\displaystyle\ln\left[1+\frac{V a r(S_{T})}{E(\widetilde{S}_{T})^{2}}\right]}{2(T-t)}\mathrm{~and~}}\\ &{\sigma^{2}=\frac{\displaystyle\ln\left[1+\frac{V a r(S_{T})}{E(S_{T})^{2}}\right]}{T-t}.}\end{array}
$$  

Figures $4.2\mathrm{a-h}$ illustrate both the probability density function as well as the cumulative distribution function of the lognormal distribution applied to some financial instrument price. The following series of figures illustrates the effect of increases in the standard deviation because different financial instruments have different levels of standard deviation. With the lognormal distribution, the input parameters in finance are typically the mean and standard deviation of the continuously compounded rates of return. Thus, the parameters are expressed in percentage and not currency units.  

We see from these graphs that with the lognormal distribution, there is no possibility that the underlying instrument value will fall to zero or below. Because common stock has limited liability, we expect the stock price to remain nonnegative, but we also expect that some stock prices will go to zero—an outcome not possible with the lognormal distribution. Based on the lognormal distribution, limited liability has no value. Thus, both the normal and lognormal distribution have strengths and weaknesses.  

![](15e3d01f75dacee41d3b42980fd9abd241ed8de25f008417076c4a34523d634b.jpg)  
FIGURE 4.2a Lognormal Probability Density Function $\dot{\mu}=0\%$ , $\sigma=30\%$ )  

![](dd6444d86747b8c7661b44531d746f5b252cc05ddb6e436b95ddcd283dcd7446.jpg)  
FIGURE 4.2b Lognormal Cumulative Distribution Function $\%=0\%$ , $\sigma=30\%$ )  

![](8601331fb400fd33c95d9654609b1051840f8de8a5afabba9bd5545df9dbf6ec.jpg)  
FIGURE 4.2c Lognormal Probability Density Function $\dot{\mu}=0\%$ , $\sigma=80\%$ )  

![](8be64da50e8c210d61f1859fbe3992205bc5fef0763c849b3dc35e42315667e0.jpg)  
FIGURE 4.2d Lognormal Cumulative Distribution Function $\%=0\%$ , $\sigma=80\%$ )  

![](707b505641efa0b6d84a0ead7a798a8ab9b66d71f090454edd75828e8f05e7b9.jpg)  
FIGURE 4.2e Lognormal Probability Density Function $:\mu=0\%$ , $\sigma=130\%$ )  

![](67c16b2a5eb042b0439b52e681cf646379900d6513fea1deaee2f9178b074423.jpg)  
FIGURE 4.2f Lognormal Cumulative Distribution Function $\%=0\%$ , $\sigma=130\%$ )  

![](c0f0cd9fde3fd7e7af9417111d8d1d1efc62e6f527841a1cc78794afa5a3a65d.jpg)  
FIGURE 4.2g Lognormal Probability Density Function $\dot{\boldsymbol{\mu}}=0\%$ , $\sigma=800\%$ )  

![](aded7ad472ae620c67c3b3fa8c80625ee21b1d48d5baeefb1947e0d11dc438ab.jpg)  
FIGURE 4.2h Lognormal Cumulative Distribution Function $(\mu=0\%$ , $\sigma=800\%$ )  

# 4.6 RECAP AND PREVIEW  

In this chapter we have provided a brief review of the foundations of probability theory, a subject widely used in understanding option valuation. We reviewed the concepts of expectation, variance, and covariance. We studied the rules for specifying these values in discrete and continuous time. We looked at the central limit theorem, the law of iterated expectations, and the law of total probability. We also examined the basic concepts behind the binomial, Poisson, normal, and lognormal distributions.  

In Chapter 5, we will look more closely at the more useful probability distributions that have financial applications.  

# QUESTIONS AND PROBLEMS  

1 Based on in-depth research, you have estimated the return distribution of two stocks for five potential scenarios that are reflected in the following table.  

<html><body><table><tr><td>Probability (%)</td><td>Stock 1 (%)</td><td>Stock 2 (%)</td></tr><tr><td>20</td><td>7</td><td>5</td></tr><tr><td>20</td><td>1</td><td>6</td></tr><tr><td>20</td><td>4</td><td>-3</td></tr><tr><td>20</td><td>4</td><td>12</td></tr><tr><td>20</td><td>-1</td><td>5</td></tr></table></body></html>  

Based on this information, compute the means and standard deviations for each stock as well as the correlation coefficient.  

2 Based on the information given in the previous problem, assume you are interested in a portfolio of $50\%$ in stock 1 and $50\%$ in stock 2. Compute the percentage marginal contribution to the risk of each stock.  

3 Suppose you are modeling an asset price with the binomial distribution where $p=$ $55\%$ (probability of the asset going up over the next year). You are interested in the likelihood that the asset will be up for two out of the three years. Identify the number of ways that this outcome could occur as well as its likelihood.  

4 The normal distribution and lognormal distribution behave very differently as the standard deviation rises. Financial instruments with high standard deviations are very common. Complete the following table based on the mean and standard deviation of the underlying normal distribution.  

<html><body><table><tr><td colspan="2">Input Normal</td><td colspan="3">Normal Distribution</td><td colspan="3">Lognormal Distribution</td></tr><tr><td>Mean (%)</td><td> Std. Dev. (%)</td><td>Mean</td><td>Median</td><td>Mode</td><td>Mean</td><td> Median</td><td>Mode</td></tr><tr><td>10</td><td>50</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>10</td><td>100</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>10</td><td>200</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>10</td><td>300</td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>  

5 Based on fundamental analysis, an analyst believes that a stock’s annualized, continuously compounded mean rate of return is $10\%$ and standard deviation is $50\%$ . Further, the analyst is comfortable with assuming a normal distribution for these rates of return. If the current stock price is 50, identify the expected stock price and standard deviation of the stock price in 10 years. Derive the implied mean and standard deviation of the normally distributed rates of return and verify your answers are correct by using your results from computing the expected stock price and standard deviation of the stock price.  

# NOTES  

1. Those who are already well-versed in probability and statistics may benefit from an alternative perspective of finance and probability as a competitive game. See Shafer and Vovk (2001).   
2. It is interesting to note that these are marginal probabilities with respect to the outcomes, $+2,0$ , $^{-2}$ , but they are joint probabilities of the coin tosses themselves. For example, the marginal probability of an outcome of $+2$ is $1/_{4}$ but this value is the joint probability of two heads, $(1/2)(1/2)=1/2$ .   
3. If we obtained a head on the first toss, we currently have a total of $+1$ . Thus, there is a probability of $^1/_{2}$ that we get another head, giving us a total of $+2$ , and a probability of $^1/_{2}$ that we get a tail, giving us a total of 0. There is zero probability that we end up with a total of 0, because we already have $+1$ .   
4. If we obtained a tail on the first toss, we currently have a total of $^{-1}$ . Thus, there is a probability of $^1/_{2}$ that we get a head, giving us a total of 0, and a probability of $^1h$ that we get another tail, giving us a total of $^{-2}$ . There is zero probability that we end up with a total of $+2$ , because we already have $^{-1}$ and have only one more toss.   
5. In finance sometimes conditional expectations are written in the form $\mathrm{E}(x_{\mathrm{t+j}}|I_{t})$ , meaning that the expectation of $x$ at time $t+j$ is based on the information set, $I,$ available at time $t$ .   
6. The standard normal distribution assumes a zero mean and standard deviation of 1. More extensive details will be given later.   
7. Note that because $\operatorname*{Pr}(x=0)=0$ , then $\operatorname*{Pr}(x\geq0)=\operatorname*{Pr}(x>0)$ .   
8. Although $f(x)$ is commonly used for the density function, $F(x)$ is often used for the distribution function.   
9. The term mean is used somewhat more often to describe data drawn from samples as opposed to the concept of expected value, which is based on ex ante information.   
10. For example, a sine wave is a nonlinear relationship that tends to show zero covariance between say $X$ and $\sin(X)$ . Nonetheless, there is clearly a relationship. It is simply nonlinear.   
11. For example, if the distribution is normal, then we can say that the probability of being more than one standard deviation away from the mean, in each direction, is about 0.16, or 0.32 for both directions.   
12. The probability mass function is the function that yields the likelihood that a discrete variable exactly equals a particular value.  