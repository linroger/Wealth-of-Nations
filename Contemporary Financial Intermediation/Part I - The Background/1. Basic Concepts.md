# Basic Concepts  

“Practical men, who believe themselves to be quite exempt from any intellectual influences, are usually the slaves of some defunct economist. Madmen in authority, who hear voices in the air, are distilling their frenzy from an academic scribbler of a few years back. I am sure that the power of vested interests is vastly exaggerated compared with the gradual encroachment of ideas.”  

John Maynard Keynes: The General Theory of Employment, Interest and Money, 1947  

# INTRODUCTION  

The modern theory of financial intermediation is based on concepts developed in financial economics. These concepts are used liberally throughout the book, so it is important to understand them well. It may not be obvious at the outset why a particular concept is needed to understand banking. For example, some may question the relevance of “market completeness” to commercial banking. Yet, this seemingly abstract concept is central to understanding financial innovation, securitization, and the off-balance sheet activities of banks. Many other concepts such as riskless arbitrage, options, market efficiency, and informational asymmetry have long shaped other subfields of finance and are transparently of great significance for a study of banking. We have thus chosen to consolidate these concepts in this chapter to provide easy reference for those who may be unfamiliar with them.  

# RISK PREFERENCES  

To understand the economic behavior of individuals, it is convenient to think of an individual as being described by a utility function that summarizes preferences over different outcomes. For a wealth level W  let $U(W)$ represent the individual’s utility of that wealth. It is reasonable to suppose that this individual always prefers more wealth to less. This is called “nonsatiation” and can be expressed as $U^{\prime}(W)>0$ , where the prime denotes a mathematical derivative. That is, at the margin, an additional unit of wealth always increases utility by some amount, however small.  

An individual can usually be classified as being either risk neutral, risk averse, or risk preferring. An individual is considered risk neutral if the individual is indifferent between the certainty of receiving the mathematical expected value of a gamble and the uncertainty of the gamble itself. Since expected wealth is relevant for the risk neutral, and the variability of wealth is not, the utility function is linear in wealth, and the second derivative, denoted $U^{\prime\prime}(W)$ , will equal zero. Letting $E(\bullet)$ denote the statistical expectation operator, we can write $U[E(W)]=E U(W)$ for a risk-neutral individual, where $U[E(W)]$ is the utility of the expected value of $W$ and $E U(W)$ is the expected utility of W  For such an individual, changing the risk of an outcome has no effect on his well-being so long as the expected outcome is left unchanged.  

The utility function of a risk-averse individual is concave in wealth, that is, $U^{\prime\prime}(W)<0$ . Such an individual prefers a certain amount to a gamble with the same expected value. Jensen’s inequality says that  

$$
U[E(W)]>E[U(W)]
$$  

if $U$ is (strictly) concave in W  Thus, risk-averse individuals prefer less risk to more, or equivalently, they demand a premium for being exposed to risk.  

A risk-preferring individual prefers the riskier of two outcomes having the same expected value. The utility function o a risk-preferring individual is convex in wealth, that is, $U^{\prime\prime}(W)>0$ , Jensen’s inequality says that  

$$
U[E(W)]<E[U(W)]
$$  

if $U$ is (strictly) convex in W  

Despite the popularity of lotteries and parimutuel betting, it is commonly assumed that individuals are risk averse. Most of finance theory is built on this assumption. Figure 1.1 depicts the different kinds of risk preferences.  

In Figure 1.2 we have drawn a picture to indicate what is going on. Consider a gamble in which an individual’s wealth W can be either $W_{1}$ with probability 0.5 or $W_{2}$ with probability 0.5. If the individual is risk averse, then the individual has a concave utility function that may look like the curve AB. Now, the individual’s expected wealth from the gamble is $E(W)=0.5W_{1}+0.5W_{2}$ which is precisely midway between $W_{1}$ and $W_{2}$ The utility derived from this expected wealth is given by $U[E(W)]$ on the $y.$ -axis. However, if this individual accepts the gamble itself [with an expected value of $E(W)]$ , then the expected utility, $E U(W)$ , is midway between $U(W_{1})$ and $U(W_{2})$ on the $y$ -axis, and can be read off the vertical axis as the point of intersection between the vertical line rising from the midpoint between $W_{1}$ and $W_{2}$ on the $x$ -axis and the straight line connecting $U(W_{1})$ and $U(W_{2})$ . Hence, as is clear from the picture, $U[E(W)]>E U(W)$ . The more bowed or concave the individual’s utility function, the more risk averse that individual will be and the larger will be the difference between $U[E(W)]$ and $E U(W)$ .  

![](images/1ba48b482c80bfc6f7178ce675642a5b955ea5428dba09294c9336f2ef38483f.jpg)  
FIGURE 1.1 Three Different Types of Utility Functions.  

![](images/990b41b813f713b29fc5144ad22feb30a4e7318738df4a830ad8670afeb9bdf4.jpg)  
FIGURE 1.2 Risk Aversion and Certainty Equivalent.  

We can also ask what sure payment we would have to offer to make this risk-averse individual indifferent between that sure payment and the gamble. Such a sure payment is known as the certainty equivalent of the gamble. In Figure 1.2  this certainty equivalent is denoted by CE on the $x$ -axis. Since the individual is risk averse, the certainty equivalent of the gamble is less than the expected value. Alternatively expressed, $E(W)$ –CE is the risk premium that the risk averse individual requires in order to participate in the gamble if his alternative is to receive CE for sure.  

The concept of risk aversion is used frequently in this book. For example, we use it in Chapter 3 to discuss the role of financial intermediaries in the economy. Risk aversion is also important in understanding financial innovation, deposit insurance, and a host of other issues.  

# DIVERSIFICATION  

We have just seen that risk-averse individuals prefer to reduce their risk. One way to reduce risk is to diversify. The basic idea behind diversification is that if you hold numerous risky assets, your return will be more predictable, but not necessarily greater. For diversification to work, it is necessary that returns on the assets in your portfolio not be perfectly and positively correlated. Indeed, if they are so correlated, the assets are identical for practical purposes so that the opportunity to diversify is defeated. Note that risk can be classified as idiosyncratic or systematic. An idiosyncratic risk is one that stems from forces specific to the asset in question, whereas systematic risk arises from the correlation of the asset’s payoff to an economy-wide phenomenon such as depression. Idiosyncratic risks are diversifiable, systematic risks are not.  

To see how diversification works, suppose that you hold two assets, A and B, whose returns are random variables.1 Let the variances of these returns be $\sigma_{\mathrm{A}}^{2}$ and $\sigma_{\mathrm{{B}}}^{2}$ , respectively. Suppose the returns on A and B are perfectly and positively correlated, so that $\rho_{\mathrm{AB}}=1$ , where $\rho_{\mathrm{AB}}$ is the correlation coefficient between A and B. The proportions of the portfolio’s value invested in A and B are $y_{\mathrm{A}}$ and $y_{\mathrm{B}}$ respectively. Then the variance of the portfolio return is  

$$
\sigma_{\mathrm{P}}^{2}=y_{\mathrm{A}}^{2}\sigma_{\mathrm{A}}^{2}+2y_{\mathrm{A}}y_{\mathrm{B}}\mathbf{Cov}(\mathrm{A,B})+y_{\mathrm{B}}^{2}\sigma_{\mathrm{B}}^{2}
$$  

where Cov(A, B) is the covariance between the returns on A and B. Then, using  

$$
\mathrm{Cov}(\mathrm{A},\mathrm{B})=\rho_{\mathrm{_{AB}}}\sigma_{\mathrm{_A}}\sigma_{\mathrm{_B}}
$$  

we have  

$$
\sigma_{\mathrm{P}}^{2}=y_{\mathrm{A}}^{2}\sigma_{\mathrm{A}}^{2}+2y_{\mathrm{A}}y_{\mathrm{B}}\rho_{{\mathrm{AB}}}\sigma_{\mathrm{A}}\sigma_{\mathrm{B}}+y_{\mathrm{B}}^{2}\sigma_{\mathrm{B}}^{2}
$$  

Since $\rho_{\mathrm{AB}}=1$ , the right-hand side of Equation (1.3) is a perfect square, $(y_{\mathrm{A}}\mathbf{\sigma}\mathbf{\sigma}\mathbf{\sigma}_{\mathrm{A}}+y_{\mathrm{B}}\mathbf{\sigma}\mathbf{\sigma}\mathbf{\sigma}_{\mathrm{B}})$ .2 As long as $y_{\mathrm{A}}\mathbf{\sigma}\mathbf{{\sigma}}_{\mathrm{A}}+y_{\mathrm{B}}\mathbf{\sigma}\mathbf{{\sigma}}_{\mathrm{B}}\geq0$ , we can write Equation (1.3) as  

$$
\sigma_{\mathrm{P}}=y_{\mathrm{A}}\sigma_{\mathrm{A}}+y_{\mathrm{B}}\sigma_{\mathrm{B}}.
$$  

Thus, if $\rho_{\mathrm{AB}}=1$ , the standard deviation of the portfolio return is just the weighted average of the standard deviations of the returns on assets A and B. Diversification therefore does not reduce portfolio risk when returns are perfectly and positively correlated. For any general correlation coefficient $\rho_{\mathrm{AB}}$ we can write the portfolio return variance as  

$$
\sigma_{\mathrm{P}}^{2}=y_{\mathrm{A}}^{2}\sigma_{\mathrm{A}}^{2}+2y_{\mathrm{A}}y_{\mathrm{B}}\rho_{{\mathrm{AB}}}\sigma_{\mathrm{A}}\sigma_{\mathrm{B}}+y_{\mathrm{B}}^{2}\sigma_{\mathrm{B}}^{2}.
$$  

Holding fixed $y_{\mathrm{A}},y_{\mathrm{B}}$ $\upsigma_{\mathrm{A}}$ and $\upsigma_{\mathrm{B}}$ we see that $\partial\sigma_{\mathrm{P}}^{2}/\partial\rho_{_{\mathrm{AB}}}>0$ , that is, portfolio risk increases with the correlation between the returns on the component assets. At $\rho_{\mathrm{AB}}=0$ (uncorrelated returns),  

$$
\sigma_{\mathrm{P}}^{2}=y_{\mathrm{A}}^{2}\sigma_{\mathrm{A}}^{2}+y_{\mathrm{B}}^{2}\sigma_{\mathrm{B}}^{2}.
$$  

# Example 1.1  

To see that diversification helps in this case, suppose $y_{\mathrm{A}}=\mathrm{y_{B}}=0.5,~\sigma_{\mathrm{A}}^{2}=100,~\sigma_{\mathrm{B}}^{2}=144.$ . Calculate the variance of a portfolio of assets A and B, assuming first that the returns of the individual assets are perfectly positively correlated, $\rho_{\mathsf{A B}}=1,$ , and then that they are uncorrelated, $\rho_{\mathsf{A B}}=0$ .  

# Solution  

n the case of perfectly and positively correlated returns, $\upsigma_{\mathrm{P}}=0.5(10)+0.5(12)=11_{.}$ , or $\sigma_{\mathrm{p}}^{2}=121$ . With uncorrelated return, Equation  1.6) implies that $\sigma_{\mathrm{p}}^{\it2}=0.25(100)+0.25(144)=61$ . Thus, not only is this variance lower than with perfectly and positively correlated returns, but it is also lower than the variance on either of the components assets.  

1  Suppose $x$ and $z$ are two random variables that can each take any value from $-\infty$ to $+\infty$ A random variable is one whose behavior is described by a probability density function, but its precise value is unknown. Let $\operatorname{f}(x)$ and $\mathrm{g}(z)$ be the density functions of $x$ and $z$ respectively. Then, the probability that $x$ will lie between the two numbers $a$ and $b$ is $\operatorname*{Pr}(a\leq x\leq b)=\int_{a}^{b}\mathrm{f}(x)\mathrm{d}x\geq0$ , and $\int_{-\infty}^{\infty}\mathrm{f}(x)\mathrm{d}x=1$ . The statistical mean (expected value) of $x$ is $\operatorname{E}(x)=\int_{-\infty}^{\infty}x\mathrm{f}(x)\mathrm{d}x$ , its variance is $\displaystyle\sigma_{x}^{2}=\int_{-\infty}^{\infty}[x-\operatorname{E}(x)]^{2}\operatorname{f}(x)\mathrm{d}x$ , and the mean and variance of $z$ are analogously defined. The covariance of $x$ and $z$ is $\mathbf{Cov}(x,z)=\int\displaylimits_{-\infty}^{\infty}\int\displaylimits_{-\infty}^{\infty}[\mathbf{x}-\mathbf{E}(\mathbf{x})][z-\mathbf{E}(z)]\mathbf{f}(x)\mathbf{g}(z)\mathrm{d}x\mathrm{d}z$ and the correlation between $x$ and $z$ is $\rho_{x z}=\mathrm{Cov}(x,z)/{\upsigma_{x}\upsigma_{z}}$ where $\upsigma_{x}$ and $\sigma_{z}$ are the standard deviations (square roots of the respective variances) of $x$ and $z$ respectively.  

The maximum effect of diversification occurs when $\rho_{\mathrm{AB}}$ is at its minimum value of $^{-1}$ , that is, returns are perfectly negatively correlated. In this case  

$$
\sigma_{\mathrm{P}}^{2}=y_{\mathrm{A}}^{2}\sigma_{\mathrm{A}}^{2}-2y_{\mathrm{A}}y_{\mathrm{B}}\sigma_{\mathrm{A}}\sigma_{\mathrm{B}}+y_{\mathrm{B}}^{2}\sigma_{\mathrm{B}}^{2}
$$  

so that  

$$
\sigma_{\mathrm{P}}=\left|y_{\mathrm{B}}\sigma_{\mathrm{B}}-y_{\mathrm{A}}\sigma_{\mathrm{A}}\right|.
$$  

This seems to indicate that the portfolio will have some risk, albeit lower than in the previous cases. But suppose we construct the portfolio so that the proportionate holdings of the assets are inversely related to their relative risks. That is,  

$$
y_{\mathrm{{A}}}/y_{\mathrm{{B}}}=\sigma_{\mathrm{{B}}}/\sigma_{\mathrm{{A}}}
$$  

or  

$$
y_{\mathrm{{A}}}=\sigma_{\mathrm{{B}}}y_{\mathrm{{B}}}/\sigma_{\mathrm{{A}}}.
$$  

Substituting Equation (1.10) in Equation (1.8) yields  

$$
\sigma_{\mathrm{P}}=y_{\mathrm{B}}\sigma_{\mathrm{B}}-(\sigma_{\mathrm{B}}y_{\mathrm{B}}\sigma_{\mathrm{A}}/\sigma_{\mathrm{A}})=0
$$  

indicating that in this special case of perfectly negatively correlated returns, portfolio risk can be reduced to zero!  

Even when assets with perfectly negatively correlated returns are unavailable, we can reduce portfolio risk by adding more assets (provided they are not perfectly positively correlated with those already in the portfolio).2 To illustrate, suppose we have $N$ assets available, each with returns pairwise uncorrelated with the returns of every other asset. In this case, a generalized version of Equation (1.6) is  

$$
\sigma_{\mathrm{P}}^{2}=\sum_{i=1}^{N}y_{\mathrm{i}}^{2}\sigma_{1}^{2}
$$  

where $y_{i}$ is the fraction of the portfolio value invested in asset $i$ where $i=1,...,N$ and $\sigma_{\mathrm{i}}^{2}$ is the variance of asset $i$ Suppose we choose $y_{\mathrm{i}}=1/N$  

Then, defining $\sigma_{\mathrm{max}}^{2}$ as the maximum variance among the $\sigma_{\mathrm{i}}^{2}$ (we assume $\sigma_{\mathrm{max}}^{2}<\infty$ , and permit $\sigma_{\mathrm{i}}^{2}=\sigma^{2}$ for all $i$ in which case $\sigma_{\mathrm{max}}^{2}=\sigma^{2}\mathrm{\tilde{\Omega}}$ ), Equation (1.11) becomes  

$$
\begin{array}{l}{{\displaystyle{\sigma}_{\mathrm{p}}^{2}=\sum_{i=1}^{N}\biggl[\frac{1}{N}\biggr]^{2}{\sigma}_{i}^{2}}}\\ {{\displaystyle{\quad\le N\biggl[\frac{1}{N}\biggr]^{2}\sigma_{\mathrm{max}}^{2}}}}\\ {{\displaystyle{\quad=\frac{\sigma_{\mathrm{max}}^{2}}{N}.}}}\end{array}
$$  

As $N$ increases, $\sigma_{\mathrm{P}}^{2}$ diminishes, and, in the limit, as $N$ goes to infinity, $\sigma_{\mathrm{P}}^{2}$ goes to zero. Thus, if we have sufficiently many assets with (pairwise) uncorrelated returns, we can drive the portfolio risk to as low as we wish and make returns as predictable as desired.  

An obvious question is why investors do not drive their risks to zero. First, not all risks are diversifiable. Some contingencies affect all assets alike and consequently holding more assets will not alter the underlying uncertainty. This is the notion of force majeure in insurance. Natural calamities such as floods and earthquakes are examples, as are losses attributed to wars. Second, as the investor increases the number of securities held in the portfolio, there are obvious costs of administration. These costs restrain diversification, but in addition numerous studies indicate that a large fraction of the potential benefits of diversification is obtained by holding a relatively small number of securities. That is, the marginal benefits of diversification decline rapidly as the number of securities increases.  

Finally, cross-sectional reusability of information diminishes the incentive to diversify. We shall have more to say in Chapter 3 about information reusability since this is a major motivation for the emergence of financial intermediaries. It suffices to say that if a lender invests in learning about a customer in the steel business in order to make a loan, it will see a potential benefit in lending to others in the steel business. The resulting concentration spreads the costs of becoming informed. Thus, we observe diversification within areas of specialization among most financial intermediaries. And when we speak of financial intermediaries processing risk, we mean that they are typically diversifying some, absorbing some, and shifting some to others.  

# RISKLESS ARBITRAGE  

Arbitrage is the simultaneous purchase and sale of identical goods or securities that are trading at disparate prices. This opportunity for riskless profit is transitory because the exploitation of such opportunities eliminates the initial price disparities.  

The term arbitrage is often loosely applied to situations in which objects of trade are similar, but not identical, and where the risk is thought to be small but not totally absent. Since such situations are often referred to as arbitrage, the redundant “riskless arbitrage” has emerged to describe arbitrage rather than limited risk speculation (a situation in which a profit can be had for a small risk). Thus, succinctly defined, riskless arbitrage is profit without risk and without investment. We shall later discuss “risk-controlled arbitrage” as an illustration of limited risk speculation. Consider the following illustration of riskless arbitrage.  

Example 1.2   
Suppose that there are two possible states of the economy next period: high (H) and low (L). Available in the capital market are two risky securities, ${\sf R}_{1}$ and ${\sf R}_{2},$ and a riskless bond, B. The state-contingent payoffs and current market prices of these instruments are presented in Table 1.1  Examine whether there are riskless arbitrage opportunities.  

TABLE 1.1 State-Contingent Payoffs and Prices of Securities   


<html><body><table><tr><td rowspan="2"></td><td colspan="2">Payoff in State</td><td rowspan="2">Current Price</td></tr><tr><td>H</td><td>L</td></tr><tr><td>Security R1</td><td>$100</td><td>0</td><td>$40</td></tr><tr><td>R2</td><td>0</td><td>$100</td><td>$40</td></tr><tr><td>B</td><td>$50</td><td>$50</td><td>$43</td></tr></table></body></html>  

# Solution  

Since you can combine ${\sf R}_{1}$ and ${\sf R}_{2}$ to get a payoff that is equivalent to that from B, you can see now that there is an opportunity for riskless arbitrage. If you buy one unit each of ${\sf R}_{1}$ and ${\sf R}_{2}$ for a total outlay of $\$80$ , you are assured of $\$100$ next period, regardless of whether state H or L is realized. So you can sell two units of B for $\$86$ earning a riskless profit of $\$6.$ You are obliged to pay the buyers of these two units of B a total of $\$100$ next period, but this you can do from the cash inflows produced by the securities ${\sf R}_{1}$ and ${\sf R}_{2}$ that you possess. Since you can sell these two units of B before you even buy ${\sf R}_{1}$ and ${\sf R}_{2}.$ your profit requires no investment on your part and no risk. You could of course sell an arbitrarily large number of units of B and buy the appropriate units of ${\sf R}_{1}$ and ${\sf R}_{2},$ giving yourself a veritable money machine. But as your purchases and sales increase in volume, it is reasonable to expect the prices of the securities to converge, thereby eliminating the opportunity for riskless arbitrage again. An important implication is that the prices of related securities cannot be determined independently of each other. This observation has provided a powerful way to price derivative securities such as options.  

The notion that any capital market equilibrium should preclude riskless arbitrage has proved to be a powerful concept in many applications in finance, including financial intermediation. We will see this idea applied in other contexts, including the valuation of contingent claims such as loan commitments.  

# OPTIONS  

An option is a contract that gives the owner the right to either buy or sell an asset at a predetermined price at some future time or over some fixed time interval. Consider an asset whose value at time $t=1$ will be $X$ Viewed at $t=0$ (the present), $X$ is a random variable. A call option entitles its owner to buy this asset at a fixed price, $P_{\mathrm{c}},$ at or before $t=1$ . If he does not wish to buy the asset, he can allow the option to expire unexercised. Thus, the value of the call option at $t=1$ is  

$$
C(t=1)=\left\{\begin{array}{l l}{X-P_{\mathrm{c}}}&{{\mathrm{if}}X>P_{\mathrm{c}}}\\ {0}&{{\mathrm{if}}X\leq P_{\mathrm{c}}.}\end{array}\right.
$$  

The theory of option pricing explains $C(t=0)$ , the value of the call option at $t=0$ . The basic idea is to construct a portfolio consisting of the underlying stock and a riskless bond in such a manner that it yields the same payoff as the option. Since there can be no riskless arbitrage in equilibrium, the prices of this portfolio should equal the price of the option. We can then price the option by using the observed prices of the stock and the bond. We will have more to say about option pricing in later chapters.  

Symmetrically, a put option entitles the option owner to sell an asset at a fixed price, $P_{\mathrm{p}},$ at or before $t=1$ . Thus, at $t=1$ the value of the put option is  

$$
P(t=1)={\left\{\begin{array}{l l}{P_{\mathrm{p}}-X}&{{\mathrm{if}}X>P_{\mathrm{p}}}\\ {0}&{{\mathrm{if}}X\leq P_{\mathrm{p}}.}\end{array}\right.}
$$  

In addition to being a put or call, an option can be either European or American. A European option can be exercised only at some predetermined maturity date, for example, at $t=1$ in the above discussion. An American option can be exercised any time prior to maturity. Thus, an American option never can be worth less than its European counterpart.  

An important property of options that we will use frequently is that the more volatile the value of the underlying security on which the option is written, the more valuable the option. The following example illustrates this property.  

# Example 1.3  

Consider a European call option with an exercise price $P_{\mathrm{c}}=\$100$ . At $t=1$ , $\chi$ will be $\$120$ with probability 0.5 and $\$90$ with probability 0.5. For simplicity, suppose everybody is risk neutral and the discount rate is zero (so that future payoffs are valued the same as current payoffs). Then from Equation  1.12) we have  

$$
C(t=1)=\left\{\begin{array}{l l}{\$10}&{\mathrm{with~probability~}0.5}\\ {0}&{\mathrm{with~probability~}0.5}\end{array}\right.
$$  

Thus, $C(t=0)=0.5(10)=\S5$ . Now suppose we increase the variance of $X,$ keeping its mean unchanged. Let $\chi$ be $\$150$ with probability 0.5 and $\$50$ with probability 0.5. From Equation  1.13) we have  

$$
C(t=1)=\left\{\begin{array}{l l}{\$50}&{\mathrm{with~probability}0.5}\\ {0}&{\mathrm{with~probability}0.5}\end{array}\right.
$$  

Thus, $C(t=0)=0.5(50)=\S25\/$ The call option is now five times more valuable! You should work through a similar example for put options to convince yourself that puts have the same property.  

Option pricing theory3 is used in our later discussions of the valuation of off-balance sheet claims like loan commitments, and in our analysis of deposit insurance.  

# MARKET EFFICIENCY  

An efficient capital market is one in which every security’s price equals its “true” economic value4  But what is true? In economics, it means a price that incorporates all the information available to investors at the time. In an efficient market, an appropriately defined set of information is fully and immediately impounded in the prices of all securities. The basic idea is that competition among investors and the resulting informational exchanges will lead to market efficiency. This implies that price changes in an efficient market must be random. If prices always reflect all relevant information, then they will change only when new information arrives. However, by definition, new information cannot be known in advance. Therefore, price changes cannot be predictable.  

We speak of three forms of market efficiency, distinguished by the amount of information impounded in the price. A market is said to be weak-form efficient if prices impound all historical information. In a weak-form efficient market, if $\boldsymbol{P}_{t}$ is the price at time $t$ then the expected value (at time $t$ ) of the price at time $t+1$ conditional on the price at time $t$ written as $E(P_{t+1}|P_{t})$ , is the same as $E(P_{t+1}|P_{t},...,P_{0})$ , the expected value of $P_{t+1}$ conditional on the entire history of stock prices up until time $t$ (that is, $P_{t},...,P_{0})$ . That is,  

$$
E(P_{_{t+1}}\mid P_{_t})=E(P_{_{t+1}}\mid P_{_t},P_{_{t-1}},P_{_{t-2}},...,P_{_0}).
$$  

This means that you can do no better forecasting tomorrow’s price $\boldsymbol{P}_{t+1}$ using the entire history of prices than you could using just today’s price $\boldsymbol{P}_{t}$ The reason is that weak-form efficiency implies that $\boldsymbol{P}_{t}$ itself should contain all the historical information contained in the sequence $\{P_{t-1}|P_{t-2},...,P_{0}\}$ .  

Semistrong form market efficiency requires that all publicly available information be contained in the current price. Since all historical information is in the public domain, a semistrong form efficient market is always weak-form efficient. However, there may be contemporaneous information in the public domain that became available after the most recent price was determined. Thus, semistrong form efficiency is a more demanding form of efficiency than weak-form efficiency. A market is strong-form efficient if prices impound all information, including that possessed by insiders. Few economists believe that markets are strong-form efficient. Although there is a mountain of empirical evidence accumulated over nearly two decades suggesting that markets are semistrong form efficient, recent theoretical and empirical research has shown that the market may not even be (always) weak-form efficient.5  

If the capital market were strong-form efficient, there would be no role for financial intermediaries as information processors (unless intermediaries were crucial in making the market efficient). However, when strong-form efficiency fails to obtain, we can have different individuals primarily possessing different sorts of information. In Chapter 3  we will show that in such markets, financial intermediaries have a role to play. At many junctures in this book, we will discuss how the efficiency (or lack thereof) of markets affects the profits to be earned from financial intermediation. An example of this is financial innovation.  

# MARKET COMPLETENESS  

The economic world we inhabit is complex and pervasively uncertain. It is often useful to think of this uncertainty in terms of the possible states of nature that can occur in the future. Each such state, call it $\theta$ can be viewed as a possible economic outcome. For example, $\theta$ may correspond to different levels of gross domestic product. Although we do not know what $\theta$ will be tomorrow, we can assign a probability distribution over possible values of $\theta$ For the theory, it does not matter how many values $\theta$ can take. For simplicity, suppose $\theta$ can take integer values from 1 to some arbitrary number $N_{\astrosun}$  

In evaluating problems of economic efficiency, an important consideration is the number of different financial securities available relative to the number of states of nature. Two financial securities are considered “different” if they do not have identical payoffs in every state. To see the implications of this, consider the following simple example.  

Example 1.4 Suppose there are three states of nature and only two securities may be thought of as shares of stock issued by two different companies. The payoffs offered by these securities in the different states of nature are shown in Table 1.2  

TABLE 1.2 Example With Three States of Nature and Two Securities   


<html><body><table><tr><td rowspan="2"></td><td colspan="3">States of Nature</td></tr><tr><td>1</td><td>2</td><td>3</td></tr><tr><td> Security 1 payoffs</td><td>10</td><td>20</td><td>15</td></tr><tr><td>Security 2 payoffs</td><td>15</td><td>0</td><td>25</td></tr></table></body></html>  

Consider now an individual who owns 10 percent of security 1 and 20 percent of security 2. If $\theta=1$ occurs, his wealth will be $0.10(10)+0.20(15)=4$ . If $\theta=2$ occurs, his wealth will be $0.10(20)+0.20(0)=2$ . If $\uptheta=3$ occurs, his wealth will be 0.10(15) $+~0.20(25)=6.5\$ . Thus, the value of the individual’s portfolio can be described by the vector (4, 2, 6.5), where the first element corresponds to his wealth in state 1 and so on.  

While the individual can achieve the vector (4, 2, 6.5), it is easy to see that one cannot achieve the vector (2, 6.5, 9.5). It is impossible for one to find ownership fractions in the two securities that will allow one to achieve this wealth vector. The reason is that there are fewer (independent) securities than there are states of nature. If we had a third security, we could have ensured that our individual could achieve any desired income vector. Of course, in reality individuals are also constrained by their budgets. The point is simply that when there are fewer securities than there are states of nature, it is generally impossible for the individual to attain any desired future wealth rearrangement. This is ultimately a limitation on the individual’s ability to insure against contingencies.  

The securities depicted in our simple example are not really stocks or bonds or any of the other financial securities commonly found in the capital market. Rather, these securities are claims to income in different states of the world. We can nevertheless visualize a market where such claims are traded. We would then have a number of securities, one for each state of nature, promising to pay 1 dollar if that particular state occurred and nothing otherwise. Such securities are called primitive state-contingent claims or Arrow-Debreu securities after the economists Kenneth Arrow and Gerard Debreu, who first studied this issue and later went on to win Nobel Prizes in Economics (Debreu, 1959)  Such a market would represent an ideal way of organizing a securities exchange, since it would give individuals complete freedom (subject only to their own purchasing power limitations) in designing portfolios that deliver the desired distribution of income in different states of the world. That is, an individual can design any “homemade” security in such a market.  

If there are as many Arrow-Debreu securities as there are states of nature, the market is referred to as complete. In a complete market, an individual can achieve any desired distribution of income, subject to the individual’s budget constraint. On the other hand, if there are fewer Arrow-Debreu securities than there are states of nature, we have an incomplete market, which places a limitation on the ability of transactors to manage uncertainty. The conceptual beauty of a complete market is that we can examine the market prices of securities that are currently trading and determine the market price of any new security we may wish to introduce. We can do this without knowing the preferences of individual investors in the economy. The key is that we can use the prices of existing securities to compute the prices of the (fictitious) Arrow-Debreu securities, and then use this information to price any new security we want to introduce. Suppose that in Example 1.2, we are given the prices of securities ${\bf R}_{1}$ and ${\bf R}_{2}$ ; recall that the price of each security is $\$40$ . Moreover, ${\bf R}_{1}$ pays off $\$100$ in state $\mathrm{~H~}$ and 0 in state L, whereas ${\bf R}_{2}$ pays off 0 in state $\mathrm{~H~}$ and $\$100$ in state L. Let $P\mathrm{i}_{\mathrm{H}}$ and $P\mathrm{i}_{\mathrm{L}}$ be the prices of the Arrow-Debreu securities in states $\mathrm{~H~}$ and L, respectively. Then, the market price of security ${\bf R}_{1}$ should be 100 times the price of the state H ArrowDebreu claim, that is, $40=100\:P\mathrm{i_{H}}$ $P\mathrm{i}_{\mathrm{H}}=0.4$ . Similarly, the market price of security ${\bf R}_{2}$ should be 100 times the price of the state $\mathrm{~L~}$ Arrow-Debreu claim, that is, $P\mathrm{i}_{\mathrm{L}}=0.4$ . We are now ready to price any security in this two-state economy. For example, the riskless bond in Example 1.2, which pays $\$50$ in each state, should be priced at $50\mathrm{{Pi}_{H}+50\mathrm{{Pi}_{L}=\S40.}}$ A security that pays $\$1,000$ in state $\mathrm{~H~}$ and $\$56$ in state $\mathrm{~L~}$ should sell at $1000P\mathrm{i}_{\mathrm{H}}+56P\mathrm{i}_{\mathrm{L}}=\mathbb{\S}422.40$ , and so on.  

The concept of market incompleteness is used in Chapter 16 in connection with our discussion of financial innovation. Other applications can be found in chapters on off-balance sheet activities, securitization, and deposit insurance.  

# ASYMMETRIC INFORMATION AND SIGNALING  

Economic transactions often involve people with different information. For example, the borrower usually knows more about its own investment opportunities than the lender does. Corporate insiders normally know more about the values of assets owned by their firms than shareholders. A doctor can be expected to be better informed about his or her own medical expertise than a patient.  

The better-informed economic agents have a natural incentive to exploit their informational advantage. Insider trading scandals on Wall Street illustrate how those with access to privileged information can profit, despite laws aimed at preventing such activity. Of course, those who are uninformed should anticipate their informational handicap and behave accordingly. It is this interaction between the inclination of the informed to strategically manipulate and the anticipation of such manipulation by the uninformed that results in distortions away from the “first best” (the economic outcome in a setting in which all are equally well-informed).  

Problems of asymmetric information were brought to the forefront when George Akerlof, who later went on to win the Nobel Prize in Economics for his contribution, sought to explain why used cars sell at such large discounts relative to the prices of new cars.6 The following example takes some shortcuts, but conveys the intuition of Akerlof’s analysis.  

Example 1.5 Consider a used car market in which differences in the care with which owners use their cars lead to quality differences among cars that started out identical. It is natural to suppose that the owner of the used car knows more about its quality than potential buyers. As an example, assume that there are three possible quality levels that the used car in question can have, $q_{1}>q_{2}>q_{3}=0$ . If the quality level is $q_{3},$ the car is a lemon. Such a car would be priced as being worthless if buyers could correctly assess its quality. If the quality is $q_{2},$ the car has a value of $\$5,$ and if the quality is $q_{1},$ the car is worth $\$10$ . Assume that all agents are risk neutral and a buyer does not want to pay more for a car than its expected worth. In a similar vein, the car owner does not wish to sell at less than what the car is worth. Suppose that each car owner knows his car’s quality, but buyers only know that cars for sale can be of quality $q_{1},q_{2},$ or $q_{3}$ Faced with a given car, they cannot identify its precise quality. However, they believe that there is a probability 0.4 that the quality is $q_{1},$ a probability 0.2 that it is $q_{2},$ and a probability 0.4 that it is $q_{3}$ What will happen in such a market?  

# Solution  

f all cars are offered for sale, risk neutral buyers will compute the expected value of a (randomly chosen) car as $(0.4)\times\$10+$ $(0.2)\times\S5+(0.4)\times0=\S5$ . Hence, if the market is competitive, we would expect $\$5$ to be the market clearing price. However, at this price those who own cars with quality $q_{1}$ will refuse to sell. Thus, only cars of qualities $q_{2}$ and $q_{3}$ will be offered at $\$5$ . However, buyers will anticipate this and revise their beliefs about the quality dispersion of cars in the market. They will now assume that if the selling price is $\$5,$ , the probability is $0.2/(0.2+0.4)=1/3$ that the quality is $q,$ and it is $2/3$ that it is ${\bf q}_{3}$ Thus, the expected value of a car drops to $(1/3)(5)+(2/3)(0)=\S1.67$ . No cars will, therefore, be bought at $\$5$ (it cannot be a market clearing price). Now if $\$1.67$ is the price, those with cars of quality $q_{2}$ will drop out and the only cars offered for sale will be lemons. This process is called adverse selection and it results in the market clearing price being driven to zero. In other words, the demand for cars at any positive price is zero, and the market breaks down, as depicted in Figure 1.3  You should note a key assumption made in this example. All market participants have rational expectations. That is, uninformed buyers rationally anticipate what informed sellers will do at any given price and informed sellers rationally anticipate the demand buyers will have at that price. Hence, we do not need to go through a sequential process of price convergence to zero. No cars will be bought or sold.  

![](images/16b878e75e2265381aecc866ad6b534c834f21087eb3655ec3f77446c0b92a1b.jpg)  
FIGURE 1.3 A Pictorial Depiction of the Adverse Selection Process.  

The insight that asymmetric information can cause market failure was novel and striking. Its profound implications were quickly recognized to extend well beyond the used car market. Informational asymmetries were seen as being capable of causing markets to break down and thus possibly justify regulatory intervention by the government. Indeed, in the chapters that follow, we will examine banking regulation from this informational perspective.  

However, calls for regulation based on Akerlof’s analysis were too hasty. Market participants have the capability and incentives to deploy mechanisms to prevent market failure, and in any case market failure is the most extreme form of distortion created by asymmetric information. To see this in the context of our used car example, consider the following extension of that example.  

Example 1.6   
Suppose that cars of different qualities have different probabilities of engine failure within a given time period, and that these differences are reflected in their values of 0, $\$5,$ , and $\$10$ . Suppose the failure probability is 0.1 for the $q_{1}$ quality car, 0.5 for the $q_{2}$ quality car, and 1 for the $q_{3}$ quality car. Do warranties have a role to play in this market?  

# Solution  

To prevent market failure, the sellers of better cars must somehow distinguish themselves from the sellers of lower quality cars. One way to do this would be with warranties or guarantees. The seller of the $q_{1}$ quality car can announce that he will reimburse the buyer $\$\mathsf{W}_{1}$ if his car fails, and the seller of $q_{2}$ quality car can announce that he will pay the buyer $\$\mathsf{W}_{1}$ if his car fails. If buyers believe that only the owners of $q_{1}$ quality cars will promise a $\$\mathsf{W}_{2}$ payment upon failure and that only the owners of $q_{2}$ quality cars will promise a $\$\mathsf{W}_{2}$ payment upon failure, then they will make the appropriate inference and should be willing to pay prices that accurately reflect the qualities of the cars offered for sale. In order for such an indirect transfer of information to be effective, no seller should wish to mimic the strategy of a seller of a different quality car. Otherwise, buyers will eventually learn of the potential mimicry and the credibility of the signal will be destroyed.  

Since the failure probability for a $q_{1}$ quality car is 0.1, the buyer should be willing to pay $\$10$ (the intrinsic worth of a $q_{1}$ quality car) plus 0.1 times $W_{1},$ the latter being the amount he expects to collect from the seller. Thus, the equilibrium price $(P_{1})$ of a $q_{1}$ quality car should be $\$10+0.1W$ Similarly, if the owner of a $q_{2}$ quality car follows his equilibrium strategy, the equilibrium price $(P_{2})$ of a $q_{2}$ quality car should be $\$5$ To ensure that the $q_{2}$ quality car owner will not misrepresent himself as a $q_{1}$ quality car owner, $W_{1}$ should be set to satisfy  

$$
\begin{array}{r}{10+0.1W_{1}-0.5W_{1}\leq5+0.5W_{2}-0.5W_{2}.}\end{array}
$$  

The left-hand side (LHS) of Equation (1.15) is the expected payoff to a $q_{2}$ quality car owner misrepresenting himself as a $q_{1}$ quality car owner; he receives a price $P_{1}$ and has an expected outflow of $0.5W_{1}$ to pay the liability under the warranty. The right-hand side (RHS) of Equation  1.15) is what the $q_{2}$ quality car owner gets if he follows his nonmimic strategy; he receives a price of $P_{2}$ and has an expected cash outflow of $0.5W_{2}$ When someone is indifferent between telling the truth and lying, it is conventionally assumed that truth-telling will be chosen. Thus, Equation (1.15), which is referred to as an  ncentive compatibility $_{\mathit{I C}}$ condition, can be treated as an equality and we can solve it to obtain $W_{1}=12.5$ . IC here means that the seller’s incentives to maximize personal profit should be compatible with truthful representation of the car’s quality.  

The IC condition that ensures that the seller of lemons does not mimic the seller of $q_{2}$ quality cars can be similarly expressed as follows  

$$
5+0.5W_{\scriptscriptstyle2}-W_{\scriptscriptstyle2}\leq0.
$$  

Solving Equation (1.16) as an equality yields $W_{2}=10$ . It is straightforward to verify that the seller of $q_{2}$ quality cars will no mimic the seller of lemons under the described conditions, that is, $q_{2}$ quality cars will be offered for sale.  

You can easily verify that this scheme guarantees that the seller of lemons will not mimic the seller of $q_{1}$ quality cars and that he seller of $q_{1}$ quality cars will not mimic either the seller of $q_{2}$ quality cars or the seller of lemons.  

To summarize, we have produced a simple scheme of “warranties” that prevents market failure. The seller of $q_{1}$ quality cars promises to pay the buyer $\$12.5$ if his car fails; this enables him to sell his car for $10+0.1(12.5)=\S11.25.$ The seller of $q_{2}$ quality cars promises to pay the buyer $\$10$ if his car fails; this enables him to sell his car for $5+0.5(10)=\$10.$ . The lemons are withdrawn from the market.  

The warranty offered here can be viewed as a signal of quality. A (perfectly revealing) signal is one that enables the uninformed to infer the information that the agent possessed privately a priori. For a signal to be useful it must be informative, and this requires that the signaling mechanism be incentive compatible. In turn, IC requires that the cost of signaling must be negatively correlated with quality.7 That is, it must be less costly at the margin for a higher quality seller to emit a given signal. The higher cost of signaling serves to deter the lower quality sellers from mimicking their higher quality counterparts. In our context, you can see that a warranty of $\$12.50$ imposes an expected liability of $\$1.25$ on the $q_{1}$ quality seller, $\$6.25$ on the $q_{2}$ quality seller, and $\$12.50$ on the seller of lemons.  

Note too that in equilibrium (i.e., when each seller maximizes expected profit) the chosen signal is costless for the seller emitting it. Although the $q_{1}$ quality seller promises to pay $\$1230$ , he has only a 0.1 probability of having to pay, and since he collects $\$11.25$ upon selling the car, his cash inflow net of the expected liability is $\$10$ . This is exactly what he would have gotten without issuing a warranty, if we were in a “first best” world in which the quality of each car was common knowledge. Likewise, the $q_{2}$ quality seller’s net cash inflow is $\$5$ . Signals are costless in equilibrium. The reason for this, as you may have guessed, is that the seller is (correctly) compensated by the buyer for issuing the warranty, that is, cars with better warranties sell at higher prices. Such signals are called nondissipative8 because the cost of the signal is a transfer payment from one party to the other, and there is no loss in the aggregate.  

We can also have dissipative signals. To see this, suppose that instead of paying cash, the seller promises to reimburse the cost of repairing a portion of the damage. The $q_{1}$ quality seller promises complete coverage, the $q_{2}$ quality seller offers to absorb half the cost of repair, and the lemons owners choose not to participate. For every dollar it costs the seller to fix the damage, its value in terms of improved car quality is $\$0.80$ . You can now easily verify that there exists a signaling scheme similar to the one derived previously which ensures truthful signaling by each seller, assuming that the seller is willing to accept a net payoff (after dissipative signaling costs are deducted) that is less than the car’s worth. The $q_{1}$ quality seller’s net receipt is less than $\$10$ and the $q_{2}$ quality seller’s is less than $\$5$ . Each absorbs a signaling cost for which it is not compensated, that is, there is a net loss due to signaling.9 For example, dividends can be a dissipative signal of future cash flows if they are personally taxed at a higher rate than capital gains (as was the case prior to the 1986 Tax Reform Act) and if external financing involves (transactions) costs that are avoided by financing with retained earnings. Later in this book, we will see other examples of dissipative signaling.  

The concept of asymmetric information underlies much of what we discuss in this book, so you should expect to encounter it in more than a few of the remaining chapters.  

# AGENCY AND MORAL HAZARD  

It has been observed that the key distinction between man and machine is moral hazard 10 First introduced in the insurance literature, this term describes situations in which the incentives of principal (the employer or the owner of the property) and agent (the employee or the person renting/using the property) diverge. A rational economic agent can be expected to maximize his own expected utility,11 and where his self-interest conflicts with the principal’s, the principal will suffer. The principal must therefore design a contract that will achieve a congruence between her goals and the agent’s.  

Examples of moral hazard abound. Consider automobile insurance. If you have a car that you know is worth $\$500$ and your collision insurance will pay you $\$1,000$ if the car is completely destroyed, you may be tempted to let your car roll down the hill and collide with an immovable object. Now you may never dream of doing this, but your willingness to spend on the maintenance of brakes may be subtly affected by your insurance policy. In any case, insurance companies cannot afford to assume that ethical or reputational considerations dominate their customers’ behaviors. This is one reason why we observe deductibles in insurance contracts. Coinsurance clauses are designed to share the risks and thereby bring the insured’s incentives into closer alignment with those of the insurer.  

Moral hazard is also common in financial contracting among claimants in a corporation. Suppose you manage a firm and your goal is to maximize shareholder wealth. If you have risky bonds outstanding, you will not always choose investments that maximize the total value of the firm. Rather, you may choose projects that maximize the value of equity at the expense of the bondholders. This can be illustrated with the following numerical example.  

# Example 1.7  

Consider a firm that will liquidate one period hence at time $\pmb{t}=1$ . There are no taxes and the firm can invest $\$30$ in a risky venture at $\pmb{t}=0$ using retained earnings. If the investment is not made, shareholders get a dividend of $\$100$ at $\pmb{t}=0$ . The firm’s debt requires a payment of $\$100$ at $\pmb{t}=1$ , and its investment choices are described in Table 1.3  

For simplicity, assume that the discount rate is zero. What should the firm do?  

TABLE 1.3 Payoffs Related to Different Investment Opportunities   


<html><body><table><tr><td rowspan="2"> Strategy</td><td colspan="2">State of Nature</td></tr><tr><td>Boom (with probability 0.5)</td><td>Bust (with probability 0.5)</td></tr><tr><td>Total firm value at t = 1 if no investment made and $100 dividendpaid at t= 0</td><td>$110</td><td>$70</td></tr><tr><td>Total firm value at t = 1 if $30 investment made and $70 dividend paid at t = 0</td><td>$200</td><td>$5</td></tr></table></body></html>  

# Solution  

To analyze this problem, first compute the net present value (NPV) of each choice for the firm as a whole. If it does not invest, then its expected value is $0.5(110)+0.5(70)=\S90$ . Add to this the $\$100$ dividend paid at $\pmb{t}=0$ and we get a total firm value of $\$190$ . If it does invest, then its expected value is $0.5(200)+0.5(5)=\S102.5$ . Add to this the $\$70$ dividend paid at $\pmb{t}=0$ and we get a total firm value of $\$172.5$ . Since the total firm value is lower with the investment than without, the project has a negative NPV. The apparent choice should be to reject the investment.  

Hold it for a minute, though! This decision rule is the right one only if you want to maximize total firm value. But remember that your goal is to maximize the wealth of the shareholders. If there is no investment, the shareholders get $\$100$ dividend plus $\$10$ ( $\$120$ debt payment) in the boom state and nothing in the bust state (limited liability, which stipulates that the liability of the shareholder does not extend beyond the assets of the firm, means that the bondholders get $\$70$ and the shareholders get $100+$ $0.5(10)=\$105$ ). On the other hand, if the project is accepted, they get $\$100$ in the boom state and nothing in the bust state. Thus, the value of this strategy to the shareholder is $70+0.5(100)=\$120$ . Clearly, the shareholders want you to invest in the project. Thus, a project with a negative NPV for the firm as a whole may be chosen in the best interest of the shareholder.  

This example illustrates a moral hazard faced by bondholders. The firm, acting in the interest of the shareholders, has an incentive to undertake investments that benefit the shareholders at the expense of creditors. In this example, the expected payoff to the bondholders is $0.5(100)+0.5(70)=\S85$ if the firm does not invest in the risky project and $0.5(100)+0.5(5)=\S52.50$ if the firm invests in the risky project. Thus, by investing in the risky project, the shareholders reduce the wealth of the bondholders by $\$32.50$ . The shareholders themselves gain $\$15,$ so that there is a net decline in total firm value of $\$17.50$ . This is the aggregate loss due to moral hazard.  

In this example, we assumed that the manager acted in the best interest of the shareholders. However, that is a questionable assumption too.12 As an agent of the shareholders, the managers can do many things that may not be in the interest of the shareholders. For example, by inflating expenses, management can divert earnings from shareholders to management. Likewise, managers can discourage takeovers and thereby entrench themselves at the possible expense of shareholders. Managers may also select myopic and low-risk investment projects with a view toward protecting their positions and reputations.  

You may have noticed that a critical assumption made in these examples is that the principal (the insurance company, the bondholders, or the shareholders) is unable to completely control the agent’s behavior. If it were possible to costlessly observe the agent’s actions, there would be no moral hazard. If the insurance company could precisely observe the insured, it would simply prohibit all actions detrimental to the car. It is because final outcomes do not unambiguously reveal the actions that may have influenced them that such proscriptions cannot be effectively written into contracts. Thus, for moral hazard to arise, the following must occur: (i) the agent’s actions (that affect the final outcome) cannot be costlessly observed by the principal, and (ii) there is some noise (exogenous uncertainty) that masks the agent’s action in the final outcome.  

Of course, the principal anticipates the agent’s behavior. Thus, the principal attempts to design a contract that aligns the agent’s incentives with it’s own. Deductibles and other coinsurance provisions in insurance contracts serve this purpose.  

Bondholders address moral hazard by limiting the firm’s debt (the higher the debt/equity ratio, the greater is the inclination of shareholders to choose risky projects), by requiring collateral, and by including in the debt contract covenants that restrict the borrower’s actions. More on this appears in Chapter 9  

Another way to address moral hazard is to contract with the agent over extended time periods. Because of the possibility of reputational consideration, the agent may restrain self-interested behavior that is to the principal’s detriment.13 However, because lives are finite and because present consumption is usually preferred to future consumption, an agent’s concern for reputation will not completely eliminate moral hazard.  

It is important to understand that moral hazard is not the same as fraud. Most interesting cases of moral hazard do not involve illegal behavior. It is not illegal for shareholders to take on riskier projects than the bondholders would like. Nor is it illegal for a manager to invest in projects with faster paybacks than shareholders would like. Moral hazard may involve fraud, but it need not. It will almost always involve ethical considerations.  

Agency and moral hazard issues, like asymmetric information, pervade much of this book. The chapters that make heaviest use of these ideas are Chapter 3 in which we discuss the role of banks and other financial intermediaries, Chapters 7 and 8 on spot lending issues, and Chapter 12 on deposit insurance.  

# TIME CONSISTENCY  

An issue that often crops up in moral hazard and adverse selection models is time consistency. To illustrate, suppose an employee expends effort to produce output on behalf of a principal. This output is affected by the agent’s effort as well as some exogenous uncertainty that is beyond the agent’s control. Thus, by observing the output the principal cannot be sure what effort the agent has taken. Suppose the principal is risk neutral and the agent is risk averse. Further, the principal must guarantee the agent some minimum level of expected utility14 to ensure his participation. Finally, the agent would rather work less than more. The sequence of events is as follows: the principal gives the agent a wage contract, after which the agent expends some effort, following which the exogenous uncertainty is resolved, and then the output is realized. How should the agent be compensated?  

If the principal could observe the agent’s effort, the answer is simple. “Optimal” risk sharing is achieved if the principal pays the agent a fixed wage conditional upon the agent expending some prespecified effort, and nothing otherwise. This risk-sharing scheme is optimal because it completely insulates the risk-averse manager from risk and imposes all of it on the risk-neutral principal. Because the effort is observable, it can be directly contracted upon. The agent will then do what the principal desires and receive a certain compensation that completely insures him against the randomness arising from the exogenous uncertainty. The principal receives the (random) output, but the randomness is costless because the principal is risk neutral.  

If the agent’s effort is unobservable, the above contract is unfeasible. The contract will be contingent on the only observable variable, the output. If the agent is promised a fixed wage, he avoids effort, so it is necessary to relate the wage to the output. This will motivate him to work harder to increase his share of the output. However, this approach to controlling the moral hazard has a cost. Since the agent is risk averse and his wage is uncertain, he will need to be compensated for the risk he bears. This will increase the principal’s wage bill.  

Now suppose that after the agent has expended his effort but before the output is realized, the principal has an opportunity to renegotiate the contract. Since the agent has already taken his effort, motivational concerns are irrelevant. The principal would be tempted to offer the agent a new wage that is fixed in amount (that is, independent of the output) and slightly less than the expected value of the agent’s wage under the old contract. The risk-averse agent will gladly accept a slight reduction in his expected wage in order to rid himself of the income uncertainty inherent in the earlier contract. The risk-neutral principal is happy to save a little on the wage bill because the risk is a matter of indifference to it. Since both the principal and the agent are happy with this new arrangement, it’s difficult to see why it would not replace the old one.  

This is an example of a wage contract that is time-inconsistent. Although it seems like a good idea to negotiate a wage contract initially which conditions the agent’s compensation on the realized output, such a contract will not work if both the agent and the principal recognize that they will subsequently want to renegotiate the effect of the contract. The possibility of renegotiating the contract destroys the incentive effect of the contract. If the agent knows that his wage ultimately will be fixed, why should he work hard? To avoid this difficulty, it is necessary to build a time-consistency (or renegotiationproofness) into the contract design. Contracts must be such that both parties to the contract should not have an incentive to renegotiate them.  

To see how renegotiation-proofness affects contracts, consider the example of a bank–borrower relationship. The bank desires to protect itself against the borrower’s incentive to increase the riskiness of the loan. It may use loan covenants that empower it to accelerate or call off the loan if the borrower violates performance standards specified in loan covenants, often expressed in terms of financial ratios. The bank believes that this threat will induce the borrower to avoid excessive risk. However, when the bank is confronted with a violation of one or more of these covenants and threatens to accelerate the loan, the borrower offers a 50 basis point increase in the loan interest rate and offers assurances that the loan covenants will remain inviolate. The bank realizes that it can increase its reported profit by accepting the borrower’s proposal and therefore withdraws its threat. To the extent that the borrower anticipates this behavior, the threat is not that the loan will be accelerated, but rather that the interest rate will be increased.  

This is an example of a loan contract that is not renegotiation-proof. A renegotiation-proof loan contract would have specified interest rate penalties for minor loan covenant violations and would have included a loan acceleration provision only for violations so egregious (and informative) that the bank’s best interest would call for the loan’s termination regardless of possible enticements by the borrower.  

Thus, contracts that are not renegotiation-proof are ultimately unsustainable. There is yet another aspect of time consistency that is unrelated to renegotiation-proofness. To illustrate, we shall use an adverse selection example. Suppose a bank is faced with two types of borrowers: good and bad. It cannot distinguish between good and bad borrowers a priori, but if it could, it would lend only to the good borrowers. Suppose the borrower incurs a cost in applying for a bank loan. Moreover, the bank can discover whether a borrower is good or bad by screening borrowers at some cost. If the bank does not screen, it charges a common interest rate from both types of borrowers and all borrowers who apply for credit. Borrowers know, however, that if the bank could distinguish among borrowers, it would lend to good borrowers exclusively. Now suppose the bank announces that it will screen all borrowers, so that it can sort out the bad borrowers and offer good borrowers a lower interest rate. Is this a time-consistent policy?  

The answer is no. If borrowers believe that the bank will implement its policy, no bad borrower would apply for credit since the application cost would be wasted. However, they will anticipate this and infer that all applicants are good. But if they are all good, why incur a screening cost? Borrowers, in turn, anticipate this and realize there will be no screening, in which case all borrowers apply. But then it pays to screen! The result is an infinite regress and there is no equilibrium. We will have more to say about this issue in our discussion of credit rationing and bank regulation.  

# NASH EQUILIBRIUM  

When agents transact with each other and each tries to selfishly maximize, they can be viewed as engaging in a noncooperative game. To describe the outcome, the concept of a Nash equilibrium has been proposed. Note first that by “equilibrium” we mean the attainment of some sort of a “steady state” in terms of the plans of action adopted by participants so that nobody can gain by unilaterally altering their plan of action. Before describing this equilibrium concept, notice that the outcome of the game depends on each player’s actions. Moreover, each individual’s actions will depend on what he thinks the adversary will do, since the final outcome is the collective resolution of individual actions. Thus, how each agent perceives the game will be played has an influence on each agent’s choice of strategy and these choices determine the final outcome. To have an equilibrium, we cannot have erroneous beliefs. That is, if I take an action believing that you will do something, then you cannot do something else; if you do, the outcome cannot be an equilibrium. I would regret having made the decision and would wish to change it.  

This intuitive notion is captured by the Nash equilibrium concept. Suppose there are $n$ players engaged in a noncooperative game. Let $\mathbf{S}_{i}$ be the strategy (choice of action) of players $i$ and let asterisks identify equilibrium strategies. Then the strategies $(\mathbf{S}_{1}^{\ast},\mathbf{S}_{2}^{\ast},\dots,\mathbf{S}_{n}^{\ast})$ constitute a Nash equilibrium if, for every $i=1,2,....,n,\mathrm{S}_{\mathrm{i}}{}^{\mathrm{:}}$ maximizes the personal welfare of agent $i$ when all other agents play their equilibrium strategies. That is, suppose players 1 and 2 are engaged in a noncooperative game and strategies ${\mathbf{S}_{1}}^{*}$ and ${\mathbf S}_{2}^{*}$ represent a Nash equilibrium. Then, holding ${\mathbf{S}_{2}}^{*}$ fixed, player 1 cannot do better with any strategy other than ${\mathbf{S}_{1}}^{*}$ , and holding ${\mathbf{S}_{1}}^{*}$ fixed, player 2 cannot do better with any strategy other than ${\mathbf{S}_{2}}^{*}$ . We now illustrate this concept in the example below and in Figure 1.4  

# Example 1.8  

Suppose there are two prisoners who jointly committed a crime. There is insufficient evidence to convict either of them, unless one or both disclose information. The police, in an attempt to break their bond of silence, separately offer each the following deal. If prisoner 1 confesses and informs on prisoner 2 (who does not confess and inform on prisoner 1), then prisoner 1 will be freed. Let 4 represent the payoff equivalent to being set free after confessing. We assume that confessing and informing on his partner in crime causes the prisoner to feel a twinge of remorse, so that he enjoys 5 if he is freed without confessing. Of course, if prisoner 1 confesses and prisoner 2 does not, the latter will be convicted. Let 0 represent the payoff equivalent of being convicted. If both prisoners confess and inform, then both will be convicted, but the person who confesses and is still convicted receives a lighter sentence than one who remains silent and is convicted. Let 1 represent the payoff equivalent of being convicted despite confessing. Both prisoners know that if neither confesses, they will both be set free. What will be the Nash equilibrium in this “prisoners’ dilemma”?  

FIGURE 1.4 Strategic Form for Prisoners’ Dilemma Game Prisoner 2.   


<html><body><table><tr><td colspan="3">Prisoner 2</td></tr><tr><td>Confess</td><td>Confess</td><td>Remain silent</td></tr><tr><td rowspan="2">Prisoner 1 Remain silent</td><td>1,1</td><td>4,0</td></tr><tr><td>0,4</td><td>5,5</td></tr></table></body></html>  

# Solution  

To answer this question let us first organize the payoffs to the various strategies in a matrix (known as the “strategic form” of this game). The first number in each cell is the payoff of prisoner 1 and the second number is the payoff of prisoner 2.  

There are two Nash equilibria in this game: (i) both prisoners confess, and (ii) both players remain silent. To see why (i) is a Nash equilibrium, suppose prisoner 1 conjectures that prisoner 2 will confess. Then, if prisoner 1 confesses he gets 1, and if he remains silent he gets 0. So he confesses. On the other hand, suppose prisoner 2 conjectures that prisoner 1 will confess. Then, since his decision problem is same as that of prisoner 1, he too finds that confessing is optimal. Thus, (i) is a Nash equilibrium because, in choosing his strategy, each prisoner correctly conjectures the strategy of the other prisoner. Similarly, if each prisoner believes that the other will remain silent, then it is clearly best for each to remain silent. Thus, (ii) is also a Nash equilibrium.  

Multiple Nash equilibria are common. Even though the two prisoners are clearly better off remaining silent, and even though they know this, it is possible for both to confess. This is because they can collude. Which equilibrium arises depends on trust among thieves.  

The concept of Nash equilibrium is used extensively in the rest of this book. In particular, you will see quite a bit of it in Chapter 3  Chapters 7 and 8  and in the discussion of bank runs and deposit insurance in Chapter 12  

# REVISION OF BELIEFS AND BAYES RULE  

In this section, we will discuss how a rational person would react to the arrival of new information. When a person does not know everything there is to know about something that will happen in the future, he can be viewed as formulating beliefs about what will happen. These beliefs can be described by a probability distribution. That is, as an incompletely informed person, you can say that you believe that there is some probability that outcome “a” will occur, some probability that outcome “b” will occur, and so on. Now, suppose some new information arrives. It does not inform you completely, but it adds to what you already know. The question is: how will you revise your original beliefs in the face of this new information? We illustrate this in the context of a specific example.  

# Example 1.9  

Suppose you wish to determine the television channel on which you should watch the evening news to learn about the next day’s weather. There are two main channels (say 1 and 2) that you can choose from. Your main criterion is the accuracy of the weather forecast, and you believe that the weather forecaster can be either $"\mathrm{good}^{\prime\prime}$ (g) or $\mathrm{\Delta^{\prime\prime}b a d^{\prime\prime}}$ (b). Right now, you think that there is a 50-50 chance that the weather forecaster on either channel is good, that is, your (prior) belief is that the probability is 0.5 that the weather forecaster is g on either channel. You also realize that nobody is perfect, so that a good forecaster has a 0.8 chance of being right and a bad forecaster has a 0.5 chance of being right. Imagine for now that the forecasters on both channels give you “point estimates” (i.e., they will tell you whether or not it will rain tomorrow) rather than probabilistic forecasts (e.g., there is a $60\%$ chance of rain). Suppose that the forecaster on channel 1 said last night that it would rain today and forecaster on channel 2 said that it would not. If you observe rain, how should you revise your beliefs?  

# Solution  

Clearly, it would not be wise to suddenly change your beliefs sharply and assert that the channel 1 forecaster is good and the channel 2 forecaster is bad. So, how should you proceed?  

To answer this question, we need to formalize the belief revision process. Bayes rule is a statistical device that provides a formula to compute how beliefs should be revised. In essence, it tells us how a rational person should compute conditional probabilities. Suppose $\smash{X_{1},...,X_{n}}$ are the possible realizations of the random variable $x$ and $\mathsf{P r}\left(x_{i}\right)$ is the prior (unconditional) probability that $x=x_{i},$ with $x_{i}$ being some value chosen from $\boldsymbol{x}_{1},...,..,{x}_{\mathrm{n}}$ Similarly, $\gamma_{i}$ is some realization of the random variable $\gamma_{\dot{\ I}}$ which conveys information about $x.$ Then, Bayes rule says that if you observe $\gamma=\gamma_{\dot{I}\prime}$ you should infer that the probability that $x=x_{i}$ is given by  

$$
\mathsf{P r}(x_{i}|y_{j})=\frac{\mathsf{P r}(y_{j}\mid x_{i})\mathsf{P r}(x_{i})}{\displaystyle\sum_{i=1}^{n}\mathsf{P r}(y_{j}\mid x_{i})\mathsf{P r}(x_{i})}
$$  

The (unconditional) probability $\mathsf{P r}(x_{i})$ is known as a prior belief and the (conditional) probability $\mathsf{P r}(x_{i}|\gamma_{j})$ is known as a posterior belief. In the context of our weather forecasting example, suppose we define  

$$
\begin{array}{r l}&{\mathrm{Pr}\left(\mathrm{forecasterisgood\left|~heiscorrect\right)}\equiv\mathrm{Pr}(\mathrm{g\midc})\right.}\\ &{\left.\mathrm{Pr}\left(\mathrm{forecasterisgood\left|~heiswrong\right)}=\mathrm{Pr}(\mathrm{g\midw})\right.\right.}\\ &{\left.\left.\mathrm{Pr}\left(\mathrm{forecasterisbad\left|~heiscorrect\right)}=\mathrm{Pr}(\mathrm{b\midc})\right.\right.}\end{array}
$$  

and so on. Then,  

Pr (channel 1 forecaster is good | he was correct in predicting rain)  

$$
{\begin{array}{r l}&{=\mathsf{P r}(\operatorname{g}|\operatorname{c})={\frac{\mathsf{P r}(\operatorname{c}|\operatorname{g})\mathsf{P r}(\operatorname{g})}{\mathsf{P r}(\operatorname{c}|\operatorname{g})\mathsf{P r}(\operatorname{g})+\mathsf{P r}(\operatorname{c}|\operatorname{b})\mathsf{P r}(\operatorname{b})}}}\\ &{={\frac{0.8\times0.5}{0.8\times0.5+0.5\times0.5}}=0.615.}\end{array}}
$$  

Similarly, Pr (channel 2 forecaster is good $\mid$ he was wrong in predicting no rain)  

$$
{\begin{array}{l}{\displaystyle=\mathsf{P r}(\mathbf{g_{\alpha}}|\mathbf{\epsilon}\mathbf{w})=\frac{\mathsf{P r}(\mathbf{w_{\alpha}}|\mathbf{g})\mathsf{P r}(\mathbf{g})}{\mathsf{P r}(\mathbf{w_{\alpha}}|\mathbf{g})\mathsf{P r}(\mathbf{g})+\mathsf{P r}(\mathbf{w_{\alpha}}|\mathbf{b})\mathsf{P r}(\mathbf{b})}}\\ {\displaystyle=\frac{0.2\times0.5}{0.2\times0.5+0.5\times0.5}}\\ {=0.286.}\end{array}}
$$  

Thus, you now think that it is more than twice as likely that the channel 1 forecaster is good compared to the channel 2 forecaster. Of course, you can wait until the next forecast and then see which (if either) of them is right. It is important to note that the latter beliefs depend in a significant way on the prior beliefs. Thus, for example, if both forecasters predict rain tonight and it does rain tomorrow, you will not say that it is equally likely that they are good; you will still believe that there is a greater likelihood that the forecaster on channel 1 is good. We will see Bayes rule at work in Chapter 8.  

# LIQUIDITY  

The liquidity of any asset has to do with the ease with which it can be converted into cash. There are three dimensions to liquidity: (i) the difference, $\Delta$ between the maximum value of the asset (typically its value to the current owner) and its value if sold; (ii) the time it takes to sell the asset at a value acceptable to the seller, $t{\mathrm{:}}$ ; and (iii) the cost involved in selling the asset, $c$ Thus, liquidity  

$$
\ell=\ell\big(\Delta,t,c\big),
$$  

and liquidity is decreasing in $\Delta,t,$ and $c$ The most liquid asset thus is cash. Generally, the less dependent the value of the asset on who owns it, the greater its liquidity. Thus, assets that are uniquely tied in value to their owners have relatively low levels of liquidity. Also, assets with high levels of asymmetric information and agency problems tend to have low levels of liquidity.  

# SYSTEMIC RISK  

Systemic risk refers to a risk that affects the whole system, and is therefore not diversifiable. It is very close in spirit and meaning to the idea of systematic risk, but the term “systemic risk” has typically been used to describe risks that affect the entire financial system, such as a system-wide shortage of liquidity.  

Traditionally, systemic risk was viewed as being completely exogenous, that is, beyond the control of any individual financial institution. But recent events have highlighted the fact that seemingly idiosyncratic risks can spread through the system and become systemic risks due to the interconnectedness of financial institutions and markets. For example, what started as defaults in the subprime mortgage market in the United States – a market that was a relatively small fraction of the global financial system – very quickly became a global financial crisis.  

# DISAGREEMENT  

In most of mainstream economics, it is standard to assume that two people confronted with the same information will always agree. That is, suppose two agents, A and B, have different beliefs at the outset about whether a project is worth investing in or a loan is worth taking, then the standard assumption is that it is because they have different information sets, and if both were provided exactly the same information, they would always agree. In other words, their beliefs would eventually converge.  

This is an assumption of convenience, however, and the idea that all agents will eventually converge to have the same beliefs has no deep support in economics, philosophy, or logic.15 A formal theory has been developed in which agents may continue to disagree even though they keep receiving the same information signals.16 The idea is as follows. Suppose two agents start out with different beliefs about the value of something, and they receive signals about some underlying economic variable that can help them update their beliefs about the value of the object. Then, it can be shown that if the probability distribution of the economic variable is nonstationary (i.e., it changes over time), then the agents’ beliefs may never converge. This means they will continue to disagree on the value of the object.  

The notion that agents may disagree, even though they have the same objective/goal (so there are no agency problems) and access to the same information (so there are no informational asymmetries) has now begun to be applied wisely in Finance and Economics to understand a variety of phenomena17  We will see this in Chapter 14 on the financial crisis of 2007–2009, when we discuss how financial innovation can lead to disagreement that can trigger a crisis.  

# MARK-TO-MARKET ACCOUNTING  

The term “MTM accounting,” refers to the practice of revising the value of an asset or liability to reflect its estimated (or actual) market value rather than its book value (which reflects the historical cost of acquiring the asset minus accumulated depreciation).  

Until the 1990s, financial institutions used book-value or historical-cost accounting, in accordance with Generally Accepted Accounting Principles (GAAP), and Regulatory Accounting Principles (RAP). Since the early 1990s, MTM has become a part of GAAP in the United States.  

Many blamed MTM for leading to fire sales and downward price spirals during the recent crisis. The argument is that financial institutions sold assets in response to diminished capital levels as a result of losses arising from marking assets to market during a period of falling prices. These asset sales then exacerbated the price decline. More on this when we turn to the 2007–2009 financial crisis in Chapter 14  

# REFERENCES  

Akerlof  G.A.  August 1970  The market for ‘lemons’: Quality uncertainty and the market mechanism  Quarter. J. Econ. 488–500   
Bhattacharya  S.  1980  Nondissipative signaling structures and dividend policy  Quarter. J. Econ. 95  1–24   
Black  F.  Scholes  M.  1973  The pricing of options and corporate liabilities  J. Polit. Econ. 81  637–654   
Bolton  P.  Dewatripont  M.  2005  Contract Theory  MIT Press  Cambridge, MA   
Boot  A.W.A.  Gopalan  R.  Thakor  A.V.  2006  The choice between private and public ownership  J. Finan. 61  803–836   
Boot  A.W.A.  Gopalan  R.  Thakor  A.V.  2008  Market liquidity, investor participation and managerial autonomy: why do firms go private? J  Finan. 63 2013–2059   
Debreu  G.  1959  Theory of Value  Cowles Foundation Monograph 17. Yale University Press  New Haven   
Fama  E.F.  1970  Efficient capital markets: A review of theory and empirical work  J. Bus. 43  383–417   
Fama  E.F.  French  Kenneth  2008  Dissecting anomalies  J. Financ. 63  1653–1678   
Holmstrom  B.  1999  Managerial incentive problems – A dynamic perspective  Rev. Econ. Stud. 66  169–182   
Jensen  M.  Meckling  W.  1976  Theory of the firm: Managerial behavior, agency costs and ownership structure  J. Financ. Econ. 3  305–360 Kreps  D.M.  1990  A Course in Microeconomic Theory  Princeton University Press  Princeton, NJ   
Kurz  M.  1994a  On rational belief equilibria  Econ. Theory 4  859–876.   
Kurz  M.  1994b  On the structure and diversity of rational beliefs  Econ. Theory 4  877–900   
Markowitz  H.  1959  Portfolio Selection: Efficient Diversification of Investments  Cowels Foundation Monograph 16 Yale University Press  New Haven Merton  R.C.  1973  Theory of rational option pricing  Bell J. Econ. Manage. Sci. 4  41–183   
Mirrlees  J.  1976  The optimal structure of incentives and authority within an organization  Bell J. Econ. 7  105–131   
Ross  S.A.  1974  On the economic theory of agency: The principle of similarity  Proceedings of the NERB-NSF Conference on Decision Making and Uncertainty   
Samuelson  P.  1963  Risk and uncertainty: A fallacy of large numbers  Scientia 57  1–6   
Spence  M.A.  1973  Job market signalling  Quarter. J. Econ.  355–374   
Spence  M.A.  1974  Competitive and optimal responses to signals: An analysis of efficiency and distribution  J. Econ. Theory 7  296–332 Schwert  W.G.  2003  Anomalies and market efficiency  In: Constantinides  G.M.  Harris  M.  Stulz  R.M. (Eds.), Handbook of Economics and Finance Elsevier Science Publishers  Amsterdam   
Tirole  J.  2006  The Theory of Corporate Finance  Princeton University Press  Princeton   
Van den Steen  E.  2010  Interpersonal authority in a theory of the firm  Am. Econ. Rev. 100  466–490  