---
title: Theory of Stabilization Policy
tags:
  - fiscal_policy
  - inside_lags
  - macroeconomic_model
  - monetary_policy
  - stabilization_policy
aliases:
  - ITC
  - Lucas critique
  - Milton Friedman
key_concepts:
  - countercyclical policy issues
  - destabilizing policy effects
  - expectations and actions
  - inside and outside lags
  - investment tax credit
  - policy intervention timing
---

# Theory of Stabilization Policy

Overview

1. In the macroeconomic model developed in the prior module, it was seen that fiscal and [[Lecture 7-Risk and Return of Bonds#7.6 Asset price reactions to monetary policy surprises|monetary policy]] could offset completely and instantaneously shocks to aggregate demand However, in practice there is uncertainty about the right magnitude and timing of policy intervention to neutralize current and projected fluctuations in income. As a result, policy can be destabilizing: you can have the right magnitude but bad timing, or you can have good timing but the wrong amount. As an example of the former destabilizing policy, a poorly-timed tax cut during a recession may boost aggregate demand after the economy already has started to boom. Thc poor timing could result from “inside lags, that is, the lags associated with recognizing the need for action and the inevitable delay in policy response; it also could result from “outside” lags, that is, the lag associated with the time between policy action and its effect on the economy. It is not just that there are inside and outside lags but thatTime there is uncertainty about the lag times; put another way, if the lag times were known with certainty then having good timing would be much easier. Other reasons for poor timing are covered in points 2 and 3 below. As an example of the latter destabilizing policy, consider a well-timed monetary action that recognizes the 12 to 18 month lag between action and maximum economic effect; also assume the policy is based on a forecast of an over-heated economy 12 to 18 months in the future. However an overly aggressive increase in interest rates may send the economy into recession and thus destabilize; this may occur because there is always some question about the size of monetary (or fiscal) change needed because of uncertainty abouf the quantitative macroeconomic impacts of any given policy action. These basic ideas were initially explored formally by Milton Friedman and are developed below beginning on page 3. Milton Friedman, “The Effects of a FullEmployment Policy on Economic Stability," in Essays in Positive Economics, 1953
2. Bad timing can result from households and firms forming expectations of policies and acting on them. Indeed a policy intended to be countercyclical can turn out to be procyclical. For example, when a temporary subsidy for the purchase of new capital goods (the so-called investment tax credit or ITC) is initially enacted to fight recessions by boosting investment demand, it probably works as businesses use the tax incentive to buy new capital goods. After being used a couple of times, its enactment probably becomes cxpected by firms who delay new investments (waiting for the ITC to be enacted) as the cconomy is turning down, exacerbating the downturn. Note that the policy action, enactment of the ITC, occurs after the initial economic effect (i.e., the delay of new investments) because enactment was anticipated. The outside lag is negative. Similarly firms likely start investing more after the ITC is enacted and as the cconomy is already on the way back up. That is, the ITC becomes procyclical exacerbating both downturns and upturns. This is a prime example of the so-called “Lucas critique" which is the idea that the initial response to a policy action occurs as soon as future implementation of the policy becomes anticipated by firms and individuals, and that response may be the opposite of desired by policymakers; at the time of implementation of the policy, the response again may be the opposite of that desired by policymakers. The profession accepts this as a very useful methodological warning, although there has bcen a substantial debate over the empirical relevance of the critique.
3. Bad timing/magnitude of policy also can result from uncertainty in real time about the meaning of incoming data on the state of the economy; indeed, data are frequently revised and some series, such as potential GDP and the associated GDP gap, are never observed yet historically have served as a basis for policy decisions. There also is uncertainty about the state of the economy when policy actions will take effect in the future (this is closely related to “outside" lags). With all this uncertainty, perhaps the vigor with which stabilization policy is pursued should be scaled back.

4. Moreover, Robert Lucas has argued that even if macroeconomic stabilization works perfectly in the sense of eliminating all fluctuations in aggregate economic activity around trend, the utility gain is minimal; if true, stabilization policy should be abandoned or at least scaled back substantially. This has led to a very lively debate with several strong counterarguments (discussed in depth below).

5. There is another argument that perfect macroeconomic stabilization is not as good an idea as one might initially think, addressed in late 2013 by former Fed Chairman Ben Bernanke. It is argued that a period of sustained economic stabilityat least partially a result of good stabilization policysuch as the Great Moderation discussed in module 1 sows the seeds of its eventual downfall. The idea is that investors, financial firms, and financial regulators during the Great Moderation (especially in the late 1990s and early 2000s) were lulled into paying insufficient attention to risks that were building, and this led to excessive risk taking that contributed to a disastrous outcome for the financial system and the overall economy

This experience raises the question of whether policy can be so successful at eliminating [[Week 3 Cyclical Industries (and Advanced Forecasting)|cyclical]] fluctuations in the economy that it leads to excessive private risk taking. I am unaware of empirical research establishing that businesses take into account changes in the capacity of policymakers to carry out successful stabilization policy in making private decisions. But even if businesses are inclined to take excessive risks during sustained periods of macroeconomic stability, it does not follow that traditional fiscal and monetary stabilization policies should be abandoned. Rather, they need to be augmented in other ways. For example, Bernanke argues that “even in stable and prosperous times, [[Lecture 7-Risk and Return of Bonds#7.6 Asset price reactions to monetary policy surprises|monetary policy]]makers and financial regulators should regard safeguarding financial stability to be of equal importance as (indeed, a necessary prerequisite for) maintaining macroeconomic stability." Along these lines, since the financial crisis of 2007-08, there has been growth in *macro-prudential policy” aimed at stabilizing financial markets; aspects of such policy are discussed in detail in my course on monetary economics (Economics 111) but won't be discussed here due to lack of time.

6. The difficulty of getting the right timing and magnitude of policy is a useful warning Indeed, aggregate demand management has been compared to hitting a moving target in a heavy fog. Nonetheless, interventionist policy can work in certain circumstances and, in any case, it is carried out in practice. Moderm Keynesians argue that fine tuning every bump and wiggle in output and employment is a bad idea but that fighting major recessions is appropriate for policy. In addition to the Lucas argument (point 4 above), RBC/classical cconomists argue that demand side policies are inappropriate given that they sce the economy as rapidly self-correcting and see fluctuations as caused by productivity/supply shocks.

Milton Friedman 's model: Formally, Friedman's argument begins as follows:

$$Z(t) =X(t) + P(t)$$ (1) where Z denotes actual (observed) GDP at time t, X denotes aggregate output (or income) at time t in the absence of a specified policy that intends to keep the economy at full cmployment; P denotes the amount of output (or income) added or subtracted from X as a result of the policy.

Note that P does not only measure the effect of the countercyclical actions taken at f. It measures the combined effect at time t of action whenever taken. It may reflect action taken much earlier or even reflect action to be taken in the future in so far as anticipation that such action will be taken affects income in period. The sum of X (i.e. output in the absence of policy) and P (output attributable to policy) is actual output, Z. The variables can be in either nominal or real terms, although it probably is better to think in real terms in order to relate this to our previous macro model. For simplicity, assume that all variables are trendless (stationary) because we are concerned with fluctuations and not levels or growth; we could equally well define the variables as deviations from trend. Finally, policy depends on the state of the cconomy in the absence of policy, i.e., P depends on X. For example, when X changes for some exogenous reason, policy can react pro-cyclically counter-cyclically, randomly, or not at all. You should think about the difficulties facing policymakers trying to react in real time.

## Magnitude and Timing

In the Friedman model, we measure the magnitude of fluctuations by the variance. Let V = $\sigma^{2}$ denote variance, where o denotes the standard deviation. In the case of P, for example, the variance of P is denoted as $V(\mathbb{P})=\varpi^{2}$p This variance would be zero if no policy action were taken. Also let p denote the simple contemporaneous correlation coefficient between X and P.

From basic probability theory, we know that $Z=X+P$ implies (you should memorize this result):
$$\begin{aligned}\mathrm{V(Z)~=~V(X)~+~V(P)~+~2~COV(X,P)~=~V(X)~+~V(P)~+~2~\rho~\sigma_X~\sigma_P}&\text{(2)}\\&=\:\sigma^{2}\:\chi\:+\:\sigma^{2}\:\rho\:+\:2\:\rho\:\sigma_{X}\:\sigma_{P}\end{aligned}$$
In this equation, $0F$ measures the magnitude of the policy intervention's effect on income. and p measures its timing or “fit" or sign. Policymakers choose p and $0F$ to minimize the fluctuations in GDP, i.e., to minimize $V(\mathbb{Z})$ technically it is to minimize the fluctuations of Z relative to its target level, as discussed in the prior section].

If countercyclical policy were always timed correctly, its effects (i.e., P) would uniformly be in the opposite direction to the deviation of X from its mean. In this case, P would be perfectly negatively correlated with X, and p would equal -1. Of course, policymakers would love this to be the case, namely that they always timed policy correctly In reality, however, policymakers do not always time their actions correctly because they do not know the complete structure of the economy and do not even know perfectly the state of the economy (due to data lags and revisions as well as imperfections in forecasting future activity, as discussed above). Humble policymakers should take this uncertainty into account, perhaps by taking actions based on imperfect timing (i.e., based on a value of p greater than -1 in algebraic terms); this is discussed more fully below.

In the case of perfect timing, i.e., if $p$ equals -1, all observations would lie on a downward-sloping straight line passing through the mean of X and P. This assumes that cycles are symmetrical about the mean. If policy were completely random in its impact, p would be zero (that is, policy is just as likely to be in the right direction as in the wrong direction). If policy were always timed perversely in the sense that its impacts were always in the same direction as the deviation of X from its mean, then p would equal +1; all observations would lie on an upward-sloping straight line running through the mean of X and P Now let's turm to some numerical examples:

Assume the full employment or natural level of income is 100 and that we want actual income $Z=100$; assume $Z_{0}=100$ Also assume $\mathsf{E}(\mathsf{P})=0$ and $\operatorname{E}(\mathbf{X})=100$.In each case below, X is assumed to take on a “boom” value of 110 and a “recession" value of 90

### Case 1: Perfect countercyclical policy

$Z = X + P$
$100 = 110 - 10$
$100 = 90 + 10$

In this case, $V(X) = V(P) = 200$ and $\rho = -1$ and $V(Z) = 0$.

### Case 2: Perfect timing and intervention is stabilizing, but too small

$105 = 110 - 5 \quad [\text{intervention is in the right direction, but } V(X) > V(P) = V(Z) > 0]$
$95 = 90 + 5$

### Case 3: Perfect timing but intervention is destabilizing

$85 = 110 - 25 \quad [V(P) > V(Z) > V(X)]$
$115 = 90 + 25$

### Case 4: Correct size but bad timing (i.e., wrong sign, with $\rho = +1$)

$120 = 110 + 10 \quad [V(Z) > V(X) = V(P)]$
$80 = 90 - 10$

From these examples, we conclude that policy is destabilizing if $V(Z) > V(X)$ and it is stabilizing if $V(Z) < V(X)$.

Under what general conditions is policy stabilizing, i.e., when does $V(Z) < V(X)$? Divide the above expression by $V(X)$:

$$
\frac{\sigma_Z^2}{\sigma_X^2} = 1 + \frac{\sigma_P^2}{\sigma_X^2} + 2\rho \frac{\sigma_P}{\sigma_X}
$$

(3)

$$
\frac{\sigma_Z^2}{\sigma_X^2} \leq 1 \quad \text{if and only if} \quad \rho \leq \frac{-\sigma_P}{2\sigma_X}
$$

Even if $V(X) = V(P)$, the policy is stabilizing only if $-1 < \rho < -1/2$, i.e., only if the policy also is properly "timed." If $V(X) = V(P)$ and $\rho = -1$, then $V(Z) = 0$. The fact that the largest negative value for $\rho$ is -1 places an upward limit on the size of the policy intervention. That is, it is possible for policy to be destabilizing even if it is perfectly "timed" with $\rho = -1$; this can happen if $\sigma_P > 2\sigma_X$, i.e., if the policy intervention is too large (case 3 above).

For a given value of $\rho$, what is the best value for $V(P)$? Is the optimum value for $V(P)$ equal to zero (i.e., do nothing) if policy is perversely timed (i.e., if the correlation is nonnegative)? To determine the optimum, differentiate the right-hand side of equation 2 with respect to $\sigma_P$ and set the result equal to zero:

$$
\sigma_P^* = -\rho \sigma_X
$$

So if $\rho = 0$, the optimum $\sigma_P$ is zero (do nothing). This also is true for $\rho > 0$.

If $\rho = -1$, the optimum is $\sigma_P = \sigma_X$ with the variance of $Z$ completely eliminated, as illustrated in the example ("perfect policy") above. *This analysis implicitly assumes that policymakers (i) know and use the true value of $\rho$ (=-1 in this case) and (ii) know and use the true value of $\sigma_X$.*

We can substitute the optimum value of $\sigma_P$ ($=-\rho \sigma_X$) into the right-hand side of equation 3 to determine the maximum reduction in variance of $X$ capable of being achieved as a function of $\rho$:

$$
\left(\frac{\sigma_Z^2}{\sigma_X^2}\right) = (1 - \rho^2)
$$

This can be re-written as:

$$
\sigma_Z^2 = (1 - \rho^2) \sigma_X^2
$$

In order to be able to cut the variance of income fluctuations, $\sigma_X^2$, in half (which would cut the standard deviation by less than a third), $\rho$ must exceed $0.7$ in absolute value, and $\sigma_P$ must be optimally chosen. If $\rho = -1/2$, $\sigma_Z^2 / \sigma_X^2$ would be $3/4$; that is, the optimal policy would set $\sigma_P$ equal to $1/2$ of $\sigma_X$ and reduce the variance of fluctuations in income by 25 percent.

This result of significant reduction in variance requires a large value of $\rho$ (in absolute value), i.e., very good timing, and requires using the optimal value for policy magnitude (i.e., $\sigma_P = -\rho \sigma_X$). Moreover, the result is *fragile* in the sense of being very sensitive to policymakers knowing and using the correct or "true" values for $\rho$ and $\sigma_X$ when making policy decisions. In reality, policymakers can be mistaken about these parameters and instead use incorrect or "false" values. An optimistic or arrogant policymaker may think they will always get the timing of policy right and so act under the assumption that $\rho = -1$ whereas a cautious, humble policymaker may use a more conservative value of $\rho$; however, both types of policymakers—especially the former—will inevitably be mistaken to some degree and end up with less than desirable outcomes for the economy.] To see how this works, let $\rho_P$ and $\rho_F$ denote the "false" and "true" values of $\rho$, respectively. Also let $\sigma_{XF}$ and $\sigma_{XT}$ denote the "true" and "false" values of the standard deviation of $X$.

It is always the case that: $\sigma _{Z}^{2}=$ $\sigma _{XT}^{2}+$ $\sigma _{P}^{2}+$ $2_{T}^{P}$ $\sigma _{XT}$ $\sigma _{P}$.In words, it is always the case that the variance of actual GDP (i.e., Z) is based on the true variance of X (the first term on the RHS), not the false reading available to policymakers; however the variance of Z also is based on the magnitude of policy $(\sigma_P)$ chosen by policymakers, which depends in this case on the false readings of $p$ and $\sigma x$

Also, recall that given p, $\sigma_{\mathrm{P}}=-\rho\sigma_{\mathrm{X}}$ which represents the optimal magnitude of policy. If policymakers use false readings on $p$ and $\sigma_{x}$ this becomes: $\sigma_{\mathrm{p}}=-P_{\mathrm{F}}\sigma_{\mathrm{XF}}$

As a specific example, assume that policymakers proceed under the mistaken assumption that timing is perfect (i.e., if they proceed thinking that $p=-1$) when in fact the true timing value is $p=-1/2$; also assume that policymakers use the true value of $\sigma x$ when choosing policy magnitude $\sigma_{\mathrm{P.}}$ Under these circumstances, the 25 percent reduction in variance of fluctuations in income (shown above) resulting from policy would be completely eliminated. This is proven by plugging $p_{\mathrm{T}}=-1/2;$ pr=-1; and $\varpi p=$ GP = $\varpi p=-\rho F\sigma xr$ into the above cquation in bold type and showing that $0z=0x$. We also can show that if the true value of p was -.49 or larger then policy would actually raise the variance of GDP and make matters worse than the case of doing nothing. It also is straightforward to prove that using any incorrect value of $p$ when choosing policy magnitude reduces the effectiveness of policy. Feel free to prove these statements. Also, I encourage you to think about what happens when a false value of $\sigma x$ but the true value of $P$ is used.

Strictly speaking, our example illustrates that using incorrect values for timing (and magnitude) by policymakers may lead to much less desirable outcomes relative to the case of using correct values. It is not unreasonable to go a bit beyond a strict interpretation of the results. One can argue that the example illustrates the general point that policymaker humility is a good thing. The policymaker who recognizes that basing policy on a very high value of timing, such as $p=$ -1 rather than on a more conservative value such as $\rho=$ 1/2 is likely to achieve a worse stabilization outcome because $\rho=-1/2$ is more likely in general to be close to the true value than is as $\rho=-1$. This is controversial because in certain circumstances such as a deep, prolonged recession or a greatly over-heated cconomyit is likely that basing policy on $p=-1$ is appropriate (this idea is developed below). Moreover, in module 4 we will consider an aspect of fiscal policy, known as the automatic stabilizers (for example, taxes decline automatically as income falls), for which i is likely that p is close to -1 because inside lags are eliminated.

Conclusions: Assume the goal of policy is to stabilize fluctuations in output (i.e. to reduce the variance of output). Policy may work well or poorly depending on the magnitude of policy $(\sigma_{\mathrm{P}})$ and on the timing (or sign) of policy (p). Also policy may work poorly if policymakers use sufficiently incorrect estimates of $p$ and $\sigma_\mathrm{P}$ in implementing policy. Assume for the remainder of the discussion that $p$ is negative (otherwise policy should be inactive with $\sigma_{\mathrm{p}}=0$) [and that policymakers use correct values for p and op.]

1. The larger the variation in X, the larger should be the policy response. This is true because the optimal value of policy magnitude is given by: $\mathbf{\sigma}_{P}^{*}=-\rho\mathbf{\sigma}_{X}$ This implies that $\mathrm{d\sigma}_{\mathbb{P}^{\prime}}$d$\varpi_\mathbb{X}=-p>0$

2. The better the timing of policy (i.e., the larger is $\beta$ in absolute value or the smaller algebraically), the larger should be the policy response to any given variation in X. Conversely, the worse the timing of policy, the smaller should be the policy response to any given variation in X. [This is proved by taking the derivative of $\mathrm{d\sigma}_{\mathrm{p}}/\mathrm{d\sigma}_{\mathrm{X}}$ with respect to p. The derivative is -1.] Policymakers should thus recognize that because policy cannot be timed perfectly, they should act more conservatively (i.e., choose a smaller value for op) than in the case of perfect timing. This result tends to support the RBC school point of view that policy should not be used much.

More generally, going a bit beyond the strict Friedman model, policymakers should act more conservatively than in the case of perfect knowledge of exactly by how much and when a policy action will impact the economy.

3. Friedman argues that it is likely the case that timing is better (i.e., p is larger in absolute value or smaller algebraically) the larger is the variation in X; intuitively a policy action is likely to be in the right direction much more frequently if action is taken to counteract only substantial movements in income rather than both substantial and tiny movements in income. In effect, Friedman argues that $p={\tilde{f}}(\sigma x$) where df/dox $\xi0$ Substitute this expression for p into the expression for the optimal magnitude of policy and get: ${\mathbf{\sigma } }_{p}^{* }= - {\mathbf{\rho } }{\mathbf{\sigma } }_{X}$ = $- {\mathbf{f} }({\mathbf{\sigma } }_{X}$) ${\mathbf{\sigma } }_{X}$. Take the derivative of o'p with respect to $\sigma x;$ the derivative is: - ox f' - p. Compare this result to that in the first point above and see that the derivative is larger here.

This implies that policy should move much more forcefully when the economy is in a deep recession than when income declines only a small amount: this supports the modern Keynesian view that fine tuning every bump and wiggle in output and employment is a bad idea but that fighting major recessions is appropriate for policy

### Does stabilization policy boost utility?

Friedman's work teaches us that you have to choose the right magnitude and timing for a countercyclical policy action to work and that this is probably difficult in real time. Subsequent research suggests thateven if the policy works perfectly, in the sense of eliminating all fluctuations in income and consumptionit may not boost utility by much at all.

This is the point of Lucas, Models of Business Cycles, Chapter II, 1987, in the context of a model of an optimizing representative consumer. There are several counterarguments that are discussed below.

To understand the Lucas result, start with a standard concave utility function (i.e., where diminishing marginal utility of consumption holds) and assume that utility is a function of the entire current and future time path of consumption. Now simplify. Suppose this is separable into the sum of utility functions of consumption at each time and ignore discounting of future utility: so, as of time t, the individual maximizes U $(\mathrm{C} _t) +$ U $(\mathrm{C} _4+ 1) +$ $\mathbf{U}(\mathbf{C}_{1+2})+\ldots$,with each component of the sum a concave function. What happens to utility at any time if the variance of consumption outcomes is eliminated?

Eliminating above-average outcomes for consumption lowers utility and eliminating belowaverage outcomes for consumption boosts utility. The gain in utility outweighs the loss assuming that the deviations are of equal magnitude-because of diminishing marginal utility of consumption (concavity), which is equivalent to the assumption that individuals are risk averse. Recall from module 2 that it was argued that the optimality of consumption smoothing implies that individuals would be willing to pay a positive amount to avoid swings in consumption over time. As will be discussed more in class, the smaller the swings in consumption, the smaller is the amount individuals would be willing to pay; also the less risk averse are individuals (i.e. the flatter is the utility curve) at realistic levels of consumption, the less are individuals willing to pay. The amount they would pay is very much like an insurance premium in this case it would be to insure against swings in total consumption. However, it is not possible to buy private insurance to protect against consumption swings due to macroeconomic events, such as recessions and booms (think about why this is so). In such circumstances, perhaps government stabilization policy could in effect provide the insurance. As we shall show next, Lacas argues that the amount individuals would pay to avoid all fluctuations in consumption-and hence the utility gain from complete stabilization is approximately zero Suppose that business cycles are symmetric about their mean (which rises over time, although this does not matter for our discussion). For the sake of concreteness, consider two extreme values for aggregate consumption: a recession level, $C_{\mathbb{R}}$ which occurs with probability %; a boom level, Cs, which occurs with probability %. Thus, the average or “normal" level of consumption, E(C), is $(C_{\mathrm{R}}+C_{\mathrm{B}})/2$.The utility of average consumption is U(E(C); this can be viewed as the utility of consumption that would hold if business cycle fluctuations were eliminated completely. Also, the average level of utility when consumption deviates from its average value, i.e., when consumption bounces between CR and $CB$ is given by $[U(C_{R})+U(C_{B})]/2$ This is exceeded in value by the utility of average consumption, U(E(C)), because of the assumption of diminishing marginal utility of consumption. Thus, if policy can eliminate all deviations in consumption from its mean value, i.e., if it can eliminate both recessions and booms, the gain in utility is: utility of stable or non-varying consumption - avg. utility if no consumption stabilization
$$=\mathrm{U(E(C))-[U(C_{R})+U(C_{B})]/2}.$$
For standard utility functions and the variance of aggregate consumption in the post-war U.S., elimination of variance increases utility by only a very small, second order, amount This is primarily because empirically the variance of aggregate consumption is a small number and because the utility curve is relatively flat at current levels of U.S. consumption. This does not mean that for any given individual that elimination of variance is virtually worthless, because some individuals may suffer large drops in consumption (utility) as a result of bad luck or unemployment, for example, and potentially could gain a lot by elimination of such downside risk. But in the aggregate, the net utility gain is small. based on Lucas* (and updated) calculations. The key implication of the Lucas analysis is that government macroeconomic stabilization policy is (approximately) worthless in the post-war United States. Thus, traditional fiscal and monetary macroeconomic stabilization policies should be abandoned.

curve). Figure 2 captures the effect of stabilization that eliminates both above and below trend movements in economic activity; all the shaded areas are eliminated, leaving simply a flat line for economic activity.

An alternative definition that considers stabilization as eliminating only below-trend movements (filling in the holes without shaving off the peaks, as discussed below and in Brad DeLong and Lawrence Summers (Brookings Papers, 1988)) in a world of symmetrical business cycles would allow an increase in the mean level of output (and consumption) and have first-order effects on utility. Figure 3 captures the effect of stabilization that eliminates only below trend movements in real economic activity; all the shaded areas are eliminated, leaving only levels of economic activity greater than or equal to the previous average level. Because the new mean exceeds the old one, some might object to referring to the exercise here as determining the potential value of “stabilization” policy and prefer to view it as the value of eliminating recessions; while the latter is perhaps more descriptively accurate, we will use the term, “stabilization” policy because many economists use the term in this way.

 ![500](Attachments/Z. Clippings/f8VpBSIggvVZIwsdffSGWapsdMkKsW5c9.png)

To make these ideas more precise, again assume that business cycles are symmetric about their mean value but policy is assumed to eliminate recessions (*filling in troughs"); in this case consumption bounces between $C_{\mathrm{B}}$ and $\mathsf{E}(\mathsf{C})=(\mathsf{C}_{\mathsf{R}}+\mathsf{C}_{\mathsf{B}})/2$ In our example, the utility gain (in expected value terms) from elimination of the recession level of consumption only is given by: avg. utility, recessions only eliminated - avg. utility if no consumption stabilization $=[U(E(C))+U(C_B)]/2$. [U$(\mathrm{C} _\mathbb{R}) +$U$(\mathrm{C} _\mathbb{B})] / 2$ =[U(E(C)) - U(C)]/2

This exceeds the utility gain of eliminating cycles completely because, in this case, boom levels of consumption are not being eliminated. Empirically, the utility gain from elimination of recessions is large; this reflects that the average level of consumption is increased from $(C_{R}+C_{B})/2$ to $[(C_{\mathrm{R}}+C_{\mathrm{B}})/2+C_{\mathrm{B}}]/2$

However, even with elimination of recessions there remains a gap between the new average value of consumption and the boom level, a gap that also could be eliminated potentially by policy. The process would continue presumably until the economy only experienced boom (or peak or capacity) levels of consumption. At this point, the issue is the value to households of elimination of occasional downward “plucks” of the cconomy from its capacity level. You can think of this as the economy normally being at $C_B$ and occasionally dropping to $C_{R}$; policy would eliminate the down-plucks and hence raise the average value of consumption to $C_{B}$ Presumably, maintaining the economy at a consumption level of $C_{\mathrm{B}}$ would result in a huge utility gain.

In the second counterargument to Lucas, national election outcomes and, indeed, the very cohesiveness of societies appear to depend on the state of the business cycle, factors generally not captured in the standard utility-maximizing framework such as the one employed by Lucas. Election outcomes throughout Europe in early 2012 and possibly in the U.S. in 2020 illustrate the sensitivity of politics and social cohesiveness to sizable actual and prospective fluctuations in economic activity (especially in the downward direction)

Third, [[Week 3 Cyclical Industries (and Advanced Forecasting)|cyclical]] downturns have a negative and, quite possibly, sizable impact on a minority of the work force; thus, stabilization policy may generate a large welfare gain for certain individuals even if the gain averaged across the entire population is small. Representative agent models (such as the model of Lucas) by construction cannot address issues raised by heterogeneity

Fourth, business-cycle variation and long-term growth may not be completely independent, as assumed by Lucas; for example, the loss of human capital associated with job loss (and possible reduction in investment in physical capital) during a [[Week 3 Cyclical Industries (and Advanced Forecasting)|cyclical]] downturn might have long-lasting adverse impacts on the growth of potential GDP and hence permanent income.

Fifth, the apparent reduction in macro variability between pre-World War II and PostWorld War II periods may be attributable to countercyclical policies; thus the elimination of stabilization policy might cause a large enough increase in the aggregate variance of output and consumption to alter the basic Lucas quantitative result.
